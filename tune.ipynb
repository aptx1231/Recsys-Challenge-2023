{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cab\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "optuna.logging.get_verbosity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-9546da201064>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test/000000000000.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m     \"\"\"\n\u001b[1;32m   1462\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test/000000000000.csv', sep='\\t')\n",
    "test_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3485852, 82)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./data/train.csv', sep='\\t')\n",
    "train_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test_data = test_data.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['f_{}'.format(i) for i in range(1, 42)]\n",
    "bin_features = ['f_{}'.format(i) for i in range(33, 42)]\n",
    "num_features = ['f_{}'.format(i) for i in range(42, 80)]\n",
    "date_features = ['f_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in cat_features:\n",
    "    le = LabelEncoder()\n",
    "    data[f] = le.fit_transform(data[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_0             0.000000\n",
      "f_1             0.038446\n",
      "f_2             0.005263\n",
      "f_3             0.290339\n",
      "f_4             0.002065\n",
      "                  ...   \n",
      "f_77            0.998190\n",
      "f_78            0.973566\n",
      "f_79            0.990726\n",
      "is_clicked      0.745728\n",
      "is_installed    0.789522\n",
      "Length: 82, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "zero_count = data.eq(0).sum()  # 计算每列中值为 0 的数量\n",
    "zero_ratio = zero_count / len(data)  # 计算每列中值为 0 的占比\n",
    "print(zero_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_ratio = zero_ratio.sort_values(ascending=False)\n",
    "zero_ratio.apply(lambda x: round(x, 5))\n",
    "zero_ratio.to_csv('./output/zero.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(num_features)):\n",
    "#     for j in range(i + 1, len(num_features)):\n",
    "#         data[f'{num_features[i]}+{num_features[j]}'] = data[num_features[i]] + data[num_features[j]]\n",
    "#         data[f'{num_features[i]}-{num_features[j]}'] = data[num_features[i]] - data[num_features[j]]\n",
    "#         data[f'{num_features[i]}*{num_features[j]}'] = data[num_features[i]] * data[num_features[j]]\n",
    "#         data[f'{num_features[i]}/{num_features[j]}'] = data[num_features[i]] / (data[num_features[j]] + 1e-7)\n",
    "# data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(bin_features)):\n",
    "#     for j in range(i + 1, len(bin_features)):\n",
    "#         data[f'{bin_features[i]}and{bin_features[j]}'] = data[bin_features[i]] & data[bin_features[j]]\n",
    "#         data[f'{bin_features[i]}or{bin_features[j]}'] = data[bin_features[i]] | data[bin_features[j]]\n",
    "#         data[f'{bin_features[i]}xor{bin_features[j]}'] = data[bin_features[i]] ^ data[bin_features[j]]\n",
    "# data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [fe for fe in data.columns if fe not in ['is_clicked', 'is_installed', 'f_0', 'label']]\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3485852, 82), (160973, 82), (3485852,), (3485852,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = data[~data['is_clicked'].isna()]\n",
    "test = data[data['is_clicked'].isna()]\n",
    "is_clicked_label = train['is_clicked'].astype('int')\n",
    "is_installed_label = train['is_installed'].astype('int')\n",
    "\n",
    "train.shape, test.shape, is_clicked_label.shape, is_installed_label.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    # 'metric': 'binary_logloss',  # auc??\n",
    "    'metric': 'binary_logloss',\n",
    "    'n_jobs': 30,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 2 ** 6,\n",
    "    'max_depth': 8,\n",
    "    'tree_learner': 'serial',\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample_freq': 1,\n",
    "    'subsample': 0.8,\n",
    "    'num_boost_round': 3000,\n",
    "    'max_bin': 255,\n",
    "    'verbose': -1,\n",
    "    'nthread' : -1,\n",
    "    'seed': seed,\n",
    "    'bagging_seed': seed,\n",
    "    'feature_fraction_seed': seed,\n",
    "    'early_stopping_rounds': 100,\n",
    "    # 'device': 'gpu',  # 设置使用 GPU 加速\n",
    "    # 'gpu_platform_id': 0,  # 设置 GPU 平台 id\n",
    "    # 'gpu_device_id': 0  # 设置 GPU 设备 id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KF = StratifiedKFold(n_splits=K, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = is_clicked_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_lgb = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Reg is_clicked\", sampler=optuna.samplers.TPESampler())\n",
    "# study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\", sampler=optuna.samplers.GridSampler())\n",
    "\n",
    "func = lambda trial: objective(trial, train, label)\n",
    "study_lgb.optimize(func, n_trials=200)\n",
    "\n",
    "print(f\"\\tBest value (LogLoss): {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = is_installed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目前使用训练集的最后一天作为评估集????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.344971\tvalid_1's binary_logloss: 0.345618\n",
      "[100]\ttraining's binary_logloss: 0.317048\tvalid_1's binary_logloss: 0.318246\n",
      "[150]\ttraining's binary_logloss: 0.305897\tvalid_1's binary_logloss: 0.30772\n",
      "[200]\ttraining's binary_logloss: 0.300174\tvalid_1's binary_logloss: 0.30274\n",
      "[250]\ttraining's binary_logloss: 0.296051\tvalid_1's binary_logloss: 0.299349\n",
      "[300]\ttraining's binary_logloss: 0.293404\tvalid_1's binary_logloss: 0.297486\n",
      "[350]\ttraining's binary_logloss: 0.291004\tvalid_1's binary_logloss: 0.295843\n",
      "[400]\ttraining's binary_logloss: 0.289048\tvalid_1's binary_logloss: 0.294642\n",
      "[450]\ttraining's binary_logloss: 0.287527\tvalid_1's binary_logloss: 0.293921\n",
      "[500]\ttraining's binary_logloss: 0.28589\tvalid_1's binary_logloss: 0.293025\n",
      "[550]\ttraining's binary_logloss: 0.284597\tvalid_1's binary_logloss: 0.292516\n",
      "[600]\ttraining's binary_logloss: 0.28335\tvalid_1's binary_logloss: 0.292028\n",
      "[650]\ttraining's binary_logloss: 0.282044\tvalid_1's binary_logloss: 0.291441\n",
      "[700]\ttraining's binary_logloss: 0.281024\tvalid_1's binary_logloss: 0.29114\n",
      "[750]\ttraining's binary_logloss: 0.280056\tvalid_1's binary_logloss: 0.290896\n",
      "[800]\ttraining's binary_logloss: 0.279139\tvalid_1's binary_logloss: 0.290672\n",
      "[850]\ttraining's binary_logloss: 0.27827\tvalid_1's binary_logloss: 0.290514\n",
      "[900]\ttraining's binary_logloss: 0.277408\tvalid_1's binary_logloss: 0.29033\n",
      "[950]\ttraining's binary_logloss: 0.276622\tvalid_1's binary_logloss: 0.290194\n",
      "[1000]\ttraining's binary_logloss: 0.275849\tvalid_1's binary_logloss: 0.29006\n",
      "[1050]\ttraining's binary_logloss: 0.275114\tvalid_1's binary_logloss: 0.289975\n",
      "[1100]\ttraining's binary_logloss: 0.274413\tvalid_1's binary_logloss: 0.289877\n",
      "[1150]\ttraining's binary_logloss: 0.273737\tvalid_1's binary_logloss: 0.289804\n",
      "[1200]\ttraining's binary_logloss: 0.273045\tvalid_1's binary_logloss: 0.289708\n",
      "[1250]\ttraining's binary_logloss: 0.272388\tvalid_1's binary_logloss: 0.289632\n",
      "[1300]\ttraining's binary_logloss: 0.271782\tvalid_1's binary_logloss: 0.289587\n",
      "[1350]\ttraining's binary_logloss: 0.271175\tvalid_1's binary_logloss: 0.289538\n",
      "[1400]\ttraining's binary_logloss: 0.270537\tvalid_1's binary_logloss: 0.289481\n",
      "[1450]\ttraining's binary_logloss: 0.269923\tvalid_1's binary_logloss: 0.289421\n",
      "[1500]\ttraining's binary_logloss: 0.269299\tvalid_1's binary_logloss: 0.28934\n",
      "[1550]\ttraining's binary_logloss: 0.268697\tvalid_1's binary_logloss: 0.289275\n",
      "[1600]\ttraining's binary_logloss: 0.268099\tvalid_1's binary_logloss: 0.289181\n",
      "[1650]\ttraining's binary_logloss: 0.267634\tvalid_1's binary_logloss: 0.289136\n",
      "[1700]\ttraining's binary_logloss: 0.267072\tvalid_1's binary_logloss: 0.289075\n",
      "[1750]\ttraining's binary_logloss: 0.266602\tvalid_1's binary_logloss: 0.289017\n",
      "[1800]\ttraining's binary_logloss: 0.266138\tvalid_1's binary_logloss: 0.288978\n",
      "[1850]\ttraining's binary_logloss: 0.265644\tvalid_1's binary_logloss: 0.28896\n",
      "[1900]\ttraining's binary_logloss: 0.265151\tvalid_1's binary_logloss: 0.288923\n",
      "[1950]\ttraining's binary_logloss: 0.264664\tvalid_1's binary_logloss: 0.288883\n",
      "[2000]\ttraining's binary_logloss: 0.264268\tvalid_1's binary_logloss: 0.28886\n",
      "[2050]\ttraining's binary_logloss: 0.263868\tvalid_1's binary_logloss: 0.288826\n",
      "[2100]\ttraining's binary_logloss: 0.263484\tvalid_1's binary_logloss: 0.288784\n",
      "[2150]\ttraining's binary_logloss: 0.263147\tvalid_1's binary_logloss: 0.288757\n",
      "[2200]\ttraining's binary_logloss: 0.262742\tvalid_1's binary_logloss: 0.288737\n",
      "[2250]\ttraining's binary_logloss: 0.262364\tvalid_1's binary_logloss: 0.288711\n",
      "[2300]\ttraining's binary_logloss: 0.262034\tvalid_1's binary_logloss: 0.288701\n",
      "[2350]\ttraining's binary_logloss: 0.261714\tvalid_1's binary_logloss: 0.288686\n",
      "[2400]\ttraining's binary_logloss: 0.261364\tvalid_1's binary_logloss: 0.288661\n",
      "[2450]\ttraining's binary_logloss: 0.261099\tvalid_1's binary_logloss: 0.288636\n",
      "[2500]\ttraining's binary_logloss: 0.26076\tvalid_1's binary_logloss: 0.288628\n",
      "[2550]\ttraining's binary_logloss: 0.260414\tvalid_1's binary_logloss: 0.288612\n",
      "[2600]\ttraining's binary_logloss: 0.260098\tvalid_1's binary_logloss: 0.288599\n",
      "[2650]\ttraining's binary_logloss: 0.259811\tvalid_1's binary_logloss: 0.288587\n",
      "[2700]\ttraining's binary_logloss: 0.259536\tvalid_1's binary_logloss: 0.288568\n",
      "[2750]\ttraining's binary_logloss: 0.259219\tvalid_1's binary_logloss: 0.288554\n",
      "[2800]\ttraining's binary_logloss: 0.258964\tvalid_1's binary_logloss: 0.288538\n",
      "[2850]\ttraining's binary_logloss: 0.258703\tvalid_1's binary_logloss: 0.288521\n",
      "[2900]\ttraining's binary_logloss: 0.258493\tvalid_1's binary_logloss: 0.288518\n",
      "[2950]\ttraining's binary_logloss: 0.258257\tvalid_1's binary_logloss: 0.288502\n",
      "[3000]\ttraining's binary_logloss: 0.25804\tvalid_1's binary_logloss: 0.288491\n",
      "[3050]\ttraining's binary_logloss: 0.25788\tvalid_1's binary_logloss: 0.288478\n",
      "[3100]\ttraining's binary_logloss: 0.257637\tvalid_1's binary_logloss: 0.288469\n",
      "[3150]\ttraining's binary_logloss: 0.257319\tvalid_1's binary_logloss: 0.288464\n",
      "[3200]\ttraining's binary_logloss: 0.257148\tvalid_1's binary_logloss: 0.288462\n",
      "[3250]\ttraining's binary_logloss: 0.256989\tvalid_1's binary_logloss: 0.288456\n",
      "[3300]\ttraining's binary_logloss: 0.256827\tvalid_1's binary_logloss: 0.288449\n",
      "[3350]\ttraining's binary_logloss: 0.256745\tvalid_1's binary_logloss: 0.288448\n",
      "[3400]\ttraining's binary_logloss: 0.256569\tvalid_1's binary_logloss: 0.288443\n",
      "[3450]\ttraining's binary_logloss: 0.256487\tvalid_1's binary_logloss: 0.288435\n",
      "[3500]\ttraining's binary_logloss: 0.25636\tvalid_1's binary_logloss: 0.28843\n",
      "[3550]\ttraining's binary_logloss: 0.256186\tvalid_1's binary_logloss: 0.288407\n",
      "[3600]\ttraining's binary_logloss: 0.256042\tvalid_1's binary_logloss: 0.288398\n",
      "[3650]\ttraining's binary_logloss: 0.255925\tvalid_1's binary_logloss: 0.288385\n",
      "[3700]\ttraining's binary_logloss: 0.255757\tvalid_1's binary_logloss: 0.288367\n",
      "[3750]\ttraining's binary_logloss: 0.255638\tvalid_1's binary_logloss: 0.288359\n",
      "[3800]\ttraining's binary_logloss: 0.255446\tvalid_1's binary_logloss: 0.288354\n",
      "[3850]\ttraining's binary_logloss: 0.255274\tvalid_1's binary_logloss: 0.288331\n",
      "[3900]\ttraining's binary_logloss: 0.255157\tvalid_1's binary_logloss: 0.288328\n",
      "[3950]\ttraining's binary_logloss: 0.255006\tvalid_1's binary_logloss: 0.288327\n",
      "[4000]\ttraining's binary_logloss: 0.25491\tvalid_1's binary_logloss: 0.288329\n",
      "[4050]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4100]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4150]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4200]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4250]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4300]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4350]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "[4400]\ttraining's binary_logloss: 0.25486\tvalid_1's binary_logloss: 0.288326\n",
      "Early stopping, best iteration is:\n",
      "[3917]\ttraining's binary_logloss: 0.255089\tvalid_1's binary_logloss: 0.288324\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.345016\tvalid_1's binary_logloss: 0.345425\n",
      "[100]\ttraining's binary_logloss: 0.317013\tvalid_1's binary_logloss: 0.318005\n",
      "[150]\ttraining's binary_logloss: 0.305751\tvalid_1's binary_logloss: 0.307377\n",
      "[200]\ttraining's binary_logloss: 0.299931\tvalid_1's binary_logloss: 0.302228\n",
      "[250]\ttraining's binary_logloss: 0.295909\tvalid_1's binary_logloss: 0.298882\n",
      "[300]\ttraining's binary_logloss: 0.293319\tvalid_1's binary_logloss: 0.29704\n",
      "[350]\ttraining's binary_logloss: 0.290968\tvalid_1's binary_logloss: 0.295401\n",
      "[400]\ttraining's binary_logloss: 0.289124\tvalid_1's binary_logloss: 0.294333\n",
      "[450]\ttraining's binary_logloss: 0.28748\tvalid_1's binary_logloss: 0.293474\n",
      "[500]\ttraining's binary_logloss: 0.285921\tvalid_1's binary_logloss: 0.292686\n",
      "[550]\ttraining's binary_logloss: 0.28456\tvalid_1's binary_logloss: 0.292117\n",
      "[600]\ttraining's binary_logloss: 0.28332\tvalid_1's binary_logloss: 0.291634\n",
      "[650]\ttraining's binary_logloss: 0.282242\tvalid_1's binary_logloss: 0.29132\n",
      "[700]\ttraining's binary_logloss: 0.281035\tvalid_1's binary_logloss: 0.29082\n",
      "[750]\ttraining's binary_logloss: 0.280066\tvalid_1's binary_logloss: 0.290555\n",
      "[800]\ttraining's binary_logloss: 0.279207\tvalid_1's binary_logloss: 0.29041\n",
      "[850]\ttraining's binary_logloss: 0.278325\tvalid_1's binary_logloss: 0.290223\n",
      "[900]\ttraining's binary_logloss: 0.277488\tvalid_1's binary_logloss: 0.290011\n",
      "[950]\ttraining's binary_logloss: 0.27671\tvalid_1's binary_logloss: 0.289907\n",
      "[1000]\ttraining's binary_logloss: 0.275922\tvalid_1's binary_logloss: 0.289764\n",
      "[1050]\ttraining's binary_logloss: 0.275213\tvalid_1's binary_logloss: 0.289687\n",
      "[1100]\ttraining's binary_logloss: 0.274408\tvalid_1's binary_logloss: 0.289496\n",
      "[1150]\ttraining's binary_logloss: 0.273718\tvalid_1's binary_logloss: 0.289359\n",
      "[1200]\ttraining's binary_logloss: 0.273038\tvalid_1's binary_logloss: 0.289301\n",
      "[1250]\ttraining's binary_logloss: 0.272356\tvalid_1's binary_logloss: 0.289206\n",
      "[1300]\ttraining's binary_logloss: 0.271675\tvalid_1's binary_logloss: 0.289088\n",
      "[1350]\ttraining's binary_logloss: 0.271047\tvalid_1's binary_logloss: 0.289001\n",
      "[1400]\ttraining's binary_logloss: 0.270477\tvalid_1's binary_logloss: 0.288951\n",
      "[1450]\ttraining's binary_logloss: 0.269907\tvalid_1's binary_logloss: 0.288914\n",
      "[1500]\ttraining's binary_logloss: 0.269399\tvalid_1's binary_logloss: 0.288894\n",
      "[1550]\ttraining's binary_logloss: 0.268867\tvalid_1's binary_logloss: 0.288858\n",
      "[1600]\ttraining's binary_logloss: 0.268336\tvalid_1's binary_logloss: 0.288825\n",
      "[1650]\ttraining's binary_logloss: 0.267854\tvalid_1's binary_logloss: 0.288798\n",
      "[1700]\ttraining's binary_logloss: 0.267303\tvalid_1's binary_logloss: 0.288736\n",
      "[1750]\ttraining's binary_logloss: 0.266753\tvalid_1's binary_logloss: 0.288666\n",
      "[1800]\ttraining's binary_logloss: 0.266302\tvalid_1's binary_logloss: 0.288645\n",
      "[1850]\ttraining's binary_logloss: 0.265868\tvalid_1's binary_logloss: 0.288618\n",
      "[1900]\ttraining's binary_logloss: 0.265386\tvalid_1's binary_logloss: 0.288602\n",
      "[1950]\ttraining's binary_logloss: 0.264968\tvalid_1's binary_logloss: 0.288567\n",
      "[2000]\ttraining's binary_logloss: 0.264531\tvalid_1's binary_logloss: 0.288539\n",
      "[2050]\ttraining's binary_logloss: 0.264099\tvalid_1's binary_logloss: 0.288521\n",
      "[2100]\ttraining's binary_logloss: 0.263728\tvalid_1's binary_logloss: 0.288504\n",
      "[2150]\ttraining's binary_logloss: 0.263378\tvalid_1's binary_logloss: 0.288471\n",
      "[2200]\ttraining's binary_logloss: 0.262996\tvalid_1's binary_logloss: 0.288464\n",
      "[2250]\ttraining's binary_logloss: 0.262722\tvalid_1's binary_logloss: 0.288456\n",
      "[2300]\ttraining's binary_logloss: 0.262445\tvalid_1's binary_logloss: 0.288444\n",
      "[2350]\ttraining's binary_logloss: 0.262046\tvalid_1's binary_logloss: 0.288406\n",
      "[2400]\ttraining's binary_logloss: 0.261663\tvalid_1's binary_logloss: 0.288387\n",
      "[2450]\ttraining's binary_logloss: 0.261271\tvalid_1's binary_logloss: 0.288376\n",
      "[2500]\ttraining's binary_logloss: 0.260907\tvalid_1's binary_logloss: 0.28836\n",
      "[2550]\ttraining's binary_logloss: 0.260584\tvalid_1's binary_logloss: 0.288326\n",
      "[2600]\ttraining's binary_logloss: 0.26034\tvalid_1's binary_logloss: 0.288297\n",
      "[2650]\ttraining's binary_logloss: 0.260089\tvalid_1's binary_logloss: 0.288277\n",
      "[2700]\ttraining's binary_logloss: 0.259874\tvalid_1's binary_logloss: 0.288273\n",
      "[2750]\ttraining's binary_logloss: 0.259603\tvalid_1's binary_logloss: 0.288242\n",
      "[2800]\ttraining's binary_logloss: 0.259379\tvalid_1's binary_logloss: 0.288238\n",
      "[2850]\ttraining's binary_logloss: 0.259175\tvalid_1's binary_logloss: 0.288229\n",
      "[2900]\ttraining's binary_logloss: 0.258956\tvalid_1's binary_logloss: 0.288224\n",
      "[2950]\ttraining's binary_logloss: 0.258761\tvalid_1's binary_logloss: 0.288222\n",
      "[3000]\ttraining's binary_logloss: 0.258521\tvalid_1's binary_logloss: 0.288207\n",
      "[3050]\ttraining's binary_logloss: 0.258394\tvalid_1's binary_logloss: 0.288199\n",
      "[3100]\ttraining's binary_logloss: 0.258167\tvalid_1's binary_logloss: 0.28819\n",
      "[3150]\ttraining's binary_logloss: 0.257936\tvalid_1's binary_logloss: 0.288166\n",
      "[3200]\ttraining's binary_logloss: 0.257722\tvalid_1's binary_logloss: 0.288158\n",
      "[3250]\ttraining's binary_logloss: 0.257608\tvalid_1's binary_logloss: 0.288154\n",
      "[3300]\ttraining's binary_logloss: 0.25738\tvalid_1's binary_logloss: 0.288129\n",
      "[3350]\ttraining's binary_logloss: 0.2572\tvalid_1's binary_logloss: 0.288113\n",
      "[3400]\ttraining's binary_logloss: 0.257065\tvalid_1's binary_logloss: 0.288108\n",
      "[3450]\ttraining's binary_logloss: 0.256928\tvalid_1's binary_logloss: 0.288093\n",
      "[3500]\ttraining's binary_logloss: 0.256787\tvalid_1's binary_logloss: 0.288085\n",
      "[3550]\ttraining's binary_logloss: 0.256599\tvalid_1's binary_logloss: 0.288065\n",
      "[3600]\ttraining's binary_logloss: 0.256456\tvalid_1's binary_logloss: 0.288062\n",
      "[3650]\ttraining's binary_logloss: 0.256298\tvalid_1's binary_logloss: 0.288059\n",
      "[3700]\ttraining's binary_logloss: 0.256184\tvalid_1's binary_logloss: 0.288056\n",
      "[3750]\ttraining's binary_logloss: 0.256006\tvalid_1's binary_logloss: 0.288049\n",
      "[3800]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[3850]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[3900]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[3950]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[4000]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[4050]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[4100]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[4150]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[4200]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "[4250]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "Early stopping, best iteration is:\n",
      "[3761]\ttraining's binary_logloss: 0.255929\tvalid_1's binary_logloss: 0.288047\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.344906\tvalid_1's binary_logloss: 0.34563\n",
      "[100]\ttraining's binary_logloss: 0.316771\tvalid_1's binary_logloss: 0.318191\n",
      "[150]\ttraining's binary_logloss: 0.305753\tvalid_1's binary_logloss: 0.307846\n",
      "[200]\ttraining's binary_logloss: 0.299776\tvalid_1's binary_logloss: 0.30262\n",
      "[250]\ttraining's binary_logloss: 0.295563\tvalid_1's binary_logloss: 0.299146\n",
      "[300]\ttraining's binary_logloss: 0.293072\tvalid_1's binary_logloss: 0.297451\n",
      "[350]\ttraining's binary_logloss: 0.290724\tvalid_1's binary_logloss: 0.295868\n",
      "[400]\ttraining's binary_logloss: 0.288747\tvalid_1's binary_logloss: 0.294671\n",
      "[450]\ttraining's binary_logloss: 0.287203\tvalid_1's binary_logloss: 0.293945\n",
      "[500]\ttraining's binary_logloss: 0.285741\tvalid_1's binary_logloss: 0.293278\n",
      "[550]\ttraining's binary_logloss: 0.284294\tvalid_1's binary_logloss: 0.292607\n",
      "[600]\ttraining's binary_logloss: 0.283155\tvalid_1's binary_logloss: 0.292256\n",
      "[650]\ttraining's binary_logloss: 0.281972\tvalid_1's binary_logloss: 0.291805\n",
      "[700]\ttraining's binary_logloss: 0.280973\tvalid_1's binary_logloss: 0.291525\n",
      "[750]\ttraining's binary_logloss: 0.279971\tvalid_1's binary_logloss: 0.291254\n",
      "[800]\ttraining's binary_logloss: 0.279021\tvalid_1's binary_logloss: 0.291021\n",
      "[850]\ttraining's binary_logloss: 0.278098\tvalid_1's binary_logloss: 0.290796\n",
      "[900]\ttraining's binary_logloss: 0.277263\tvalid_1's binary_logloss: 0.290638\n",
      "[950]\ttraining's binary_logloss: 0.276441\tvalid_1's binary_logloss: 0.290473\n",
      "[1000]\ttraining's binary_logloss: 0.27567\tvalid_1's binary_logloss: 0.290346\n",
      "[1050]\ttraining's binary_logloss: 0.274854\tvalid_1's binary_logloss: 0.290167\n",
      "[1100]\ttraining's binary_logloss: 0.274126\tvalid_1's binary_logloss: 0.29007\n",
      "[1150]\ttraining's binary_logloss: 0.273446\tvalid_1's binary_logloss: 0.289982\n",
      "[1200]\ttraining's binary_logloss: 0.272781\tvalid_1's binary_logloss: 0.289908\n",
      "[1250]\ttraining's binary_logloss: 0.272129\tvalid_1's binary_logloss: 0.289841\n",
      "[1300]\ttraining's binary_logloss: 0.271474\tvalid_1's binary_logloss: 0.289776\n",
      "[1350]\ttraining's binary_logloss: 0.270844\tvalid_1's binary_logloss: 0.289718\n",
      "[1400]\ttraining's binary_logloss: 0.270219\tvalid_1's binary_logloss: 0.289641\n",
      "[1450]\ttraining's binary_logloss: 0.269641\tvalid_1's binary_logloss: 0.289603\n",
      "[1500]\ttraining's binary_logloss: 0.269046\tvalid_1's binary_logloss: 0.289556\n",
      "[1550]\ttraining's binary_logloss: 0.268519\tvalid_1's binary_logloss: 0.289525\n",
      "[1600]\ttraining's binary_logloss: 0.267988\tvalid_1's binary_logloss: 0.289477\n",
      "[1650]\ttraining's binary_logloss: 0.267452\tvalid_1's binary_logloss: 0.289444\n",
      "[1700]\ttraining's binary_logloss: 0.266926\tvalid_1's binary_logloss: 0.289401\n",
      "[1750]\ttraining's binary_logloss: 0.266481\tvalid_1's binary_logloss: 0.289371\n",
      "[1800]\ttraining's binary_logloss: 0.265954\tvalid_1's binary_logloss: 0.289299\n",
      "[1850]\ttraining's binary_logloss: 0.265489\tvalid_1's binary_logloss: 0.289282\n",
      "[1900]\ttraining's binary_logloss: 0.265018\tvalid_1's binary_logloss: 0.289263\n",
      "[1950]\ttraining's binary_logloss: 0.264544\tvalid_1's binary_logloss: 0.289241\n",
      "[2000]\ttraining's binary_logloss: 0.264132\tvalid_1's binary_logloss: 0.289203\n",
      "[2050]\ttraining's binary_logloss: 0.263729\tvalid_1's binary_logloss: 0.289191\n",
      "[2100]\ttraining's binary_logloss: 0.263375\tvalid_1's binary_logloss: 0.289164\n",
      "[2150]\ttraining's binary_logloss: 0.26301\tvalid_1's binary_logloss: 0.289137\n",
      "[2200]\ttraining's binary_logloss: 0.262576\tvalid_1's binary_logloss: 0.28907\n",
      "[2250]\ttraining's binary_logloss: 0.26214\tvalid_1's binary_logloss: 0.289055\n",
      "[2300]\ttraining's binary_logloss: 0.26181\tvalid_1's binary_logloss: 0.289043\n",
      "[2350]\ttraining's binary_logloss: 0.261431\tvalid_1's binary_logloss: 0.289019\n",
      "[2400]\ttraining's binary_logloss: 0.261145\tvalid_1's binary_logloss: 0.289005\n",
      "[2450]\ttraining's binary_logloss: 0.260871\tvalid_1's binary_logloss: 0.288999\n",
      "[2500]\ttraining's binary_logloss: 0.260535\tvalid_1's binary_logloss: 0.288989\n",
      "[2550]\ttraining's binary_logloss: 0.260224\tvalid_1's binary_logloss: 0.288985\n",
      "[2600]\ttraining's binary_logloss: 0.259922\tvalid_1's binary_logloss: 0.288964\n",
      "[2650]\ttraining's binary_logloss: 0.259599\tvalid_1's binary_logloss: 0.288944\n",
      "[2700]\ttraining's binary_logloss: 0.259316\tvalid_1's binary_logloss: 0.288921\n",
      "[2750]\ttraining's binary_logloss: 0.259041\tvalid_1's binary_logloss: 0.288917\n",
      "[2800]\ttraining's binary_logloss: 0.25877\tvalid_1's binary_logloss: 0.288911\n",
      "[2850]\ttraining's binary_logloss: 0.258514\tvalid_1's binary_logloss: 0.288893\n",
      "[2900]\ttraining's binary_logloss: 0.258288\tvalid_1's binary_logloss: 0.288892\n",
      "[2950]\ttraining's binary_logloss: 0.25805\tvalid_1's binary_logloss: 0.288889\n",
      "[3000]\ttraining's binary_logloss: 0.257788\tvalid_1's binary_logloss: 0.288884\n",
      "[3050]\ttraining's binary_logloss: 0.257566\tvalid_1's binary_logloss: 0.288875\n",
      "[3100]\ttraining's binary_logloss: 0.257372\tvalid_1's binary_logloss: 0.288875\n",
      "[3150]\ttraining's binary_logloss: 0.257161\tvalid_1's binary_logloss: 0.288873\n",
      "[3200]\ttraining's binary_logloss: 0.25695\tvalid_1's binary_logloss: 0.28887\n",
      "[3250]\ttraining's binary_logloss: 0.256733\tvalid_1's binary_logloss: 0.288856\n",
      "[3300]\ttraining's binary_logloss: 0.256535\tvalid_1's binary_logloss: 0.288838\n",
      "[3350]\ttraining's binary_logloss: 0.256299\tvalid_1's binary_logloss: 0.288821\n",
      "[3400]\ttraining's binary_logloss: 0.256166\tvalid_1's binary_logloss: 0.28882\n",
      "[3450]\ttraining's binary_logloss: 0.255981\tvalid_1's binary_logloss: 0.288824\n",
      "[3500]\ttraining's binary_logloss: 0.255842\tvalid_1's binary_logloss: 0.288809\n",
      "[3550]\ttraining's binary_logloss: 0.255646\tvalid_1's binary_logloss: 0.288807\n",
      "[3600]\ttraining's binary_logloss: 0.255455\tvalid_1's binary_logloss: 0.288807\n",
      "[3650]\ttraining's binary_logloss: 0.255367\tvalid_1's binary_logloss: 0.288813\n",
      "[3700]\ttraining's binary_logloss: 0.255252\tvalid_1's binary_logloss: 0.288805\n",
      "[3750]\ttraining's binary_logloss: 0.255129\tvalid_1's binary_logloss: 0.288802\n",
      "[3800]\ttraining's binary_logloss: 0.254966\tvalid_1's binary_logloss: 0.2888\n",
      "[3850]\ttraining's binary_logloss: 0.254784\tvalid_1's binary_logloss: 0.288802\n",
      "[3900]\ttraining's binary_logloss: 0.254655\tvalid_1's binary_logloss: 0.288798\n",
      "[3950]\ttraining's binary_logloss: 0.254506\tvalid_1's binary_logloss: 0.288796\n",
      "[4000]\ttraining's binary_logloss: 0.254427\tvalid_1's binary_logloss: 0.288798\n",
      "[4050]\ttraining's binary_logloss: 0.254373\tvalid_1's binary_logloss: 0.288799\n",
      "[4100]\ttraining's binary_logloss: 0.254194\tvalid_1's binary_logloss: 0.288799\n",
      "[4150]\ttraining's binary_logloss: 0.254068\tvalid_1's binary_logloss: 0.288789\n",
      "[4200]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4250]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4300]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4350]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4400]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4450]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4500]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4550]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4600]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "[4650]\ttraining's binary_logloss: 0.254025\tvalid_1's binary_logloss: 0.288792\n",
      "Early stopping, best iteration is:\n",
      "[4161]\ttraining's binary_logloss: 0.254052\tvalid_1's binary_logloss: 0.288788\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.345002\tvalid_1's binary_logloss: 0.345668\n",
      "[100]\ttraining's binary_logloss: 0.316894\tvalid_1's binary_logloss: 0.318142\n",
      "[150]\ttraining's binary_logloss: 0.305775\tvalid_1's binary_logloss: 0.307639\n",
      "[200]\ttraining's binary_logloss: 0.300119\tvalid_1's binary_logloss: 0.302672\n",
      "[250]\ttraining's binary_logloss: 0.296145\tvalid_1's binary_logloss: 0.299426\n",
      "[300]\ttraining's binary_logloss: 0.293359\tvalid_1's binary_logloss: 0.297401\n",
      "[350]\ttraining's binary_logloss: 0.291119\tvalid_1's binary_logloss: 0.295942\n",
      "[400]\ttraining's binary_logloss: 0.289347\tvalid_1's binary_logloss: 0.294981\n",
      "[450]\ttraining's binary_logloss: 0.287757\tvalid_1's binary_logloss: 0.294209\n",
      "[500]\ttraining's binary_logloss: 0.286259\tvalid_1's binary_logloss: 0.293516\n",
      "[550]\ttraining's binary_logloss: 0.284863\tvalid_1's binary_logloss: 0.292898\n",
      "[600]\ttraining's binary_logloss: 0.283576\tvalid_1's binary_logloss: 0.292364\n",
      "[650]\ttraining's binary_logloss: 0.282479\tvalid_1's binary_logloss: 0.292047\n",
      "[700]\ttraining's binary_logloss: 0.281336\tvalid_1's binary_logloss: 0.291641\n",
      "[750]\ttraining's binary_logloss: 0.280359\tvalid_1's binary_logloss: 0.291419\n",
      "[800]\ttraining's binary_logloss: 0.2795\tvalid_1's binary_logloss: 0.291254\n",
      "[850]\ttraining's binary_logloss: 0.278579\tvalid_1's binary_logloss: 0.291014\n",
      "[900]\ttraining's binary_logloss: 0.27777\tvalid_1's binary_logloss: 0.290857\n",
      "[950]\ttraining's binary_logloss: 0.276795\tvalid_1's binary_logloss: 0.290528\n",
      "[1000]\ttraining's binary_logloss: 0.276001\tvalid_1's binary_logloss: 0.290391\n",
      "[1050]\ttraining's binary_logloss: 0.275272\tvalid_1's binary_logloss: 0.290284\n",
      "[1100]\ttraining's binary_logloss: 0.274551\tvalid_1's binary_logloss: 0.290207\n",
      "[1150]\ttraining's binary_logloss: 0.273843\tvalid_1's binary_logloss: 0.290086\n",
      "[1200]\ttraining's binary_logloss: 0.273223\tvalid_1's binary_logloss: 0.290048\n",
      "[1250]\ttraining's binary_logloss: 0.272534\tvalid_1's binary_logloss: 0.289945\n",
      "[1300]\ttraining's binary_logloss: 0.271914\tvalid_1's binary_logloss: 0.289879\n",
      "[1350]\ttraining's binary_logloss: 0.271294\tvalid_1's binary_logloss: 0.289814\n",
      "[1400]\ttraining's binary_logloss: 0.270672\tvalid_1's binary_logloss: 0.289738\n",
      "[1450]\ttraining's binary_logloss: 0.270143\tvalid_1's binary_logloss: 0.289711\n",
      "[1500]\ttraining's binary_logloss: 0.269602\tvalid_1's binary_logloss: 0.289677\n",
      "[1550]\ttraining's binary_logloss: 0.269044\tvalid_1's binary_logloss: 0.289637\n",
      "[1600]\ttraining's binary_logloss: 0.268479\tvalid_1's binary_logloss: 0.289572\n",
      "[1650]\ttraining's binary_logloss: 0.267953\tvalid_1's binary_logloss: 0.289545\n",
      "[1700]\ttraining's binary_logloss: 0.267486\tvalid_1's binary_logloss: 0.289493\n",
      "[1750]\ttraining's binary_logloss: 0.266993\tvalid_1's binary_logloss: 0.289466\n",
      "[1800]\ttraining's binary_logloss: 0.266496\tvalid_1's binary_logloss: 0.289426\n",
      "[1850]\ttraining's binary_logloss: 0.266025\tvalid_1's binary_logloss: 0.289397\n",
      "[1900]\ttraining's binary_logloss: 0.265599\tvalid_1's binary_logloss: 0.289377\n",
      "[1950]\ttraining's binary_logloss: 0.265168\tvalid_1's binary_logloss: 0.289347\n",
      "[2000]\ttraining's binary_logloss: 0.264708\tvalid_1's binary_logloss: 0.289308\n",
      "[2050]\ttraining's binary_logloss: 0.264289\tvalid_1's binary_logloss: 0.289273\n",
      "[2100]\ttraining's binary_logloss: 0.263855\tvalid_1's binary_logloss: 0.289242\n",
      "[2150]\ttraining's binary_logloss: 0.263407\tvalid_1's binary_logloss: 0.28918\n",
      "[2200]\ttraining's binary_logloss: 0.26303\tvalid_1's binary_logloss: 0.289153\n",
      "[2250]\ttraining's binary_logloss: 0.262704\tvalid_1's binary_logloss: 0.289139\n",
      "[2300]\ttraining's binary_logloss: 0.262279\tvalid_1's binary_logloss: 0.289116\n",
      "[2350]\ttraining's binary_logloss: 0.26191\tvalid_1's binary_logloss: 0.289099\n",
      "[2400]\ttraining's binary_logloss: 0.261526\tvalid_1's binary_logloss: 0.289064\n",
      "[2450]\ttraining's binary_logloss: 0.261139\tvalid_1's binary_logloss: 0.289039\n",
      "[2500]\ttraining's binary_logloss: 0.260825\tvalid_1's binary_logloss: 0.289026\n",
      "[2550]\ttraining's binary_logloss: 0.260479\tvalid_1's binary_logloss: 0.289019\n",
      "[2600]\ttraining's binary_logloss: 0.260189\tvalid_1's binary_logloss: 0.288991\n",
      "[2650]\ttraining's binary_logloss: 0.259915\tvalid_1's binary_logloss: 0.288973\n",
      "[2700]\ttraining's binary_logloss: 0.259669\tvalid_1's binary_logloss: 0.288957\n",
      "[2750]\ttraining's binary_logloss: 0.259435\tvalid_1's binary_logloss: 0.288943\n",
      "[2800]\ttraining's binary_logloss: 0.259116\tvalid_1's binary_logloss: 0.288907\n",
      "[2850]\ttraining's binary_logloss: 0.258863\tvalid_1's binary_logloss: 0.288898\n",
      "[2900]\ttraining's binary_logloss: 0.258684\tvalid_1's binary_logloss: 0.288883\n",
      "[2950]\ttraining's binary_logloss: 0.258475\tvalid_1's binary_logloss: 0.288879\n",
      "[3000]\ttraining's binary_logloss: 0.258244\tvalid_1's binary_logloss: 0.288859\n",
      "[3050]\ttraining's binary_logloss: 0.258085\tvalid_1's binary_logloss: 0.288857\n",
      "[3100]\ttraining's binary_logloss: 0.257913\tvalid_1's binary_logloss: 0.288854\n",
      "[3150]\ttraining's binary_logloss: 0.257586\tvalid_1's binary_logloss: 0.288799\n",
      "[3200]\ttraining's binary_logloss: 0.257395\tvalid_1's binary_logloss: 0.28879\n",
      "[3250]\ttraining's binary_logloss: 0.25722\tvalid_1's binary_logloss: 0.288783\n",
      "[3300]\ttraining's binary_logloss: 0.25698\tvalid_1's binary_logloss: 0.288779\n",
      "[3350]\ttraining's binary_logloss: 0.256752\tvalid_1's binary_logloss: 0.288774\n",
      "[3400]\ttraining's binary_logloss: 0.2566\tvalid_1's binary_logloss: 0.288766\n",
      "[3450]\ttraining's binary_logloss: 0.256475\tvalid_1's binary_logloss: 0.288756\n",
      "[3500]\ttraining's binary_logloss: 0.256328\tvalid_1's binary_logloss: 0.288752\n",
      "[3550]\ttraining's binary_logloss: 0.256219\tvalid_1's binary_logloss: 0.28874\n",
      "[3600]\ttraining's binary_logloss: 0.256028\tvalid_1's binary_logloss: 0.288739\n",
      "[3650]\ttraining's binary_logloss: 0.255847\tvalid_1's binary_logloss: 0.288727\n",
      "[3700]\ttraining's binary_logloss: 0.255748\tvalid_1's binary_logloss: 0.288721\n",
      "[3750]\ttraining's binary_logloss: 0.255628\tvalid_1's binary_logloss: 0.288713\n",
      "[3800]\ttraining's binary_logloss: 0.255497\tvalid_1's binary_logloss: 0.28871\n",
      "[3850]\ttraining's binary_logloss: 0.255349\tvalid_1's binary_logloss: 0.288704\n",
      "[3900]\ttraining's binary_logloss: 0.255153\tvalid_1's binary_logloss: 0.288684\n",
      "[3950]\ttraining's binary_logloss: 0.25501\tvalid_1's binary_logloss: 0.288685\n",
      "[4000]\ttraining's binary_logloss: 0.254836\tvalid_1's binary_logloss: 0.288679\n",
      "[4050]\ttraining's binary_logloss: 0.254696\tvalid_1's binary_logloss: 0.288677\n",
      "[4100]\ttraining's binary_logloss: 0.254547\tvalid_1's binary_logloss: 0.288676\n",
      "[4150]\ttraining's binary_logloss: 0.254433\tvalid_1's binary_logloss: 0.288668\n",
      "[4200]\ttraining's binary_logloss: 0.254248\tvalid_1's binary_logloss: 0.288669\n",
      "[4250]\ttraining's binary_logloss: 0.254095\tvalid_1's binary_logloss: 0.288672\n",
      "[4300]\ttraining's binary_logloss: 0.253974\tvalid_1's binary_logloss: 0.288669\n",
      "[4350]\ttraining's binary_logloss: 0.253831\tvalid_1's binary_logloss: 0.288676\n",
      "[4400]\ttraining's binary_logloss: 0.253715\tvalid_1's binary_logloss: 0.288675\n",
      "[4450]\ttraining's binary_logloss: 0.253601\tvalid_1's binary_logloss: 0.288651\n",
      "[4500]\ttraining's binary_logloss: 0.253473\tvalid_1's binary_logloss: 0.288643\n",
      "[4550]\ttraining's binary_logloss: 0.253363\tvalid_1's binary_logloss: 0.288611\n",
      "[4600]\ttraining's binary_logloss: 0.25326\tvalid_1's binary_logloss: 0.28861\n",
      "[4650]\ttraining's binary_logloss: 0.253192\tvalid_1's binary_logloss: 0.288604\n",
      "[4700]\ttraining's binary_logloss: 0.253061\tvalid_1's binary_logloss: 0.288603\n",
      "[4750]\ttraining's binary_logloss: 0.252943\tvalid_1's binary_logloss: 0.288601\n",
      "[4800]\ttraining's binary_logloss: 0.252858\tvalid_1's binary_logloss: 0.2886\n",
      "[4850]\ttraining's binary_logloss: 0.252741\tvalid_1's binary_logloss: 0.288603\n",
      "[4900]\ttraining's binary_logloss: 0.25265\tvalid_1's binary_logloss: 0.288601\n",
      "[4950]\ttraining's binary_logloss: 0.25256\tvalid_1's binary_logloss: 0.2886\n",
      "[5000]\ttraining's binary_logloss: 0.252457\tvalid_1's binary_logloss: 0.288602\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.252457\tvalid_1's binary_logloss: 0.288602\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.345006\tvalid_1's binary_logloss: 0.345925\n",
      "[100]\ttraining's binary_logloss: 0.317018\tvalid_1's binary_logloss: 0.318632\n",
      "[150]\ttraining's binary_logloss: 0.305654\tvalid_1's binary_logloss: 0.307915\n",
      "[200]\ttraining's binary_logloss: 0.300011\tvalid_1's binary_logloss: 0.302992\n",
      "[250]\ttraining's binary_logloss: 0.295868\tvalid_1's binary_logloss: 0.299568\n",
      "[300]\ttraining's binary_logloss: 0.293102\tvalid_1's binary_logloss: 0.297576\n",
      "[350]\ttraining's binary_logloss: 0.290811\tvalid_1's binary_logloss: 0.296068\n",
      "[400]\ttraining's binary_logloss: 0.288893\tvalid_1's binary_logloss: 0.294942\n",
      "[450]\ttraining's binary_logloss: 0.287198\tvalid_1's binary_logloss: 0.294049\n",
      "[500]\ttraining's binary_logloss: 0.285667\tvalid_1's binary_logloss: 0.29329\n",
      "[550]\ttraining's binary_logloss: 0.284368\tvalid_1's binary_logloss: 0.292794\n",
      "[600]\ttraining's binary_logloss: 0.283012\tvalid_1's binary_logloss: 0.292168\n",
      "[650]\ttraining's binary_logloss: 0.281941\tvalid_1's binary_logloss: 0.291882\n",
      "[700]\ttraining's binary_logloss: 0.280929\tvalid_1's binary_logloss: 0.291606\n",
      "[750]\ttraining's binary_logloss: 0.279973\tvalid_1's binary_logloss: 0.291396\n",
      "[800]\ttraining's binary_logloss: 0.279101\tvalid_1's binary_logloss: 0.291229\n",
      "[850]\ttraining's binary_logloss: 0.278198\tvalid_1's binary_logloss: 0.290994\n",
      "[900]\ttraining's binary_logloss: 0.277389\tvalid_1's binary_logloss: 0.290847\n",
      "[950]\ttraining's binary_logloss: 0.276558\tvalid_1's binary_logloss: 0.290672\n",
      "[1000]\ttraining's binary_logloss: 0.275786\tvalid_1's binary_logloss: 0.29055\n",
      "[1050]\ttraining's binary_logloss: 0.275036\tvalid_1's binary_logloss: 0.290455\n",
      "[1100]\ttraining's binary_logloss: 0.27434\tvalid_1's binary_logloss: 0.290376\n",
      "[1150]\ttraining's binary_logloss: 0.273688\tvalid_1's binary_logloss: 0.29031\n",
      "[1200]\ttraining's binary_logloss: 0.273042\tvalid_1's binary_logloss: 0.290239\n",
      "[1250]\ttraining's binary_logloss: 0.272393\tvalid_1's binary_logloss: 0.290171\n",
      "[1300]\ttraining's binary_logloss: 0.271765\tvalid_1's binary_logloss: 0.290106\n",
      "[1350]\ttraining's binary_logloss: 0.271124\tvalid_1's binary_logloss: 0.29001\n",
      "[1400]\ttraining's binary_logloss: 0.270522\tvalid_1's binary_logloss: 0.289969\n",
      "[1450]\ttraining's binary_logloss: 0.27\tvalid_1's binary_logloss: 0.289938\n",
      "[1500]\ttraining's binary_logloss: 0.269406\tvalid_1's binary_logloss: 0.289895\n",
      "[1550]\ttraining's binary_logloss: 0.268827\tvalid_1's binary_logloss: 0.289831\n",
      "[1600]\ttraining's binary_logloss: 0.268256\tvalid_1's binary_logloss: 0.289772\n",
      "[1650]\ttraining's binary_logloss: 0.267697\tvalid_1's binary_logloss: 0.289682\n",
      "[1700]\ttraining's binary_logloss: 0.267172\tvalid_1's binary_logloss: 0.289637\n",
      "[1750]\ttraining's binary_logloss: 0.266636\tvalid_1's binary_logloss: 0.289591\n",
      "[1800]\ttraining's binary_logloss: 0.266124\tvalid_1's binary_logloss: 0.289564\n",
      "[1850]\ttraining's binary_logloss: 0.265641\tvalid_1's binary_logloss: 0.289541\n",
      "[1900]\ttraining's binary_logloss: 0.265145\tvalid_1's binary_logloss: 0.289491\n",
      "[1950]\ttraining's binary_logloss: 0.264693\tvalid_1's binary_logloss: 0.289464\n",
      "[2000]\ttraining's binary_logloss: 0.264244\tvalid_1's binary_logloss: 0.289425\n",
      "[2050]\ttraining's binary_logloss: 0.263785\tvalid_1's binary_logloss: 0.289394\n",
      "[2100]\ttraining's binary_logloss: 0.263356\tvalid_1's binary_logloss: 0.289366\n",
      "[2150]\ttraining's binary_logloss: 0.262955\tvalid_1's binary_logloss: 0.289348\n",
      "[2200]\ttraining's binary_logloss: 0.262692\tvalid_1's binary_logloss: 0.289332\n",
      "[2250]\ttraining's binary_logloss: 0.262348\tvalid_1's binary_logloss: 0.289305\n",
      "[2300]\ttraining's binary_logloss: 0.262013\tvalid_1's binary_logloss: 0.289289\n",
      "[2350]\ttraining's binary_logloss: 0.261604\tvalid_1's binary_logloss: 0.289277\n",
      "[2400]\ttraining's binary_logloss: 0.261265\tvalid_1's binary_logloss: 0.28926\n",
      "[2450]\ttraining's binary_logloss: 0.261034\tvalid_1's binary_logloss: 0.289244\n",
      "[2500]\ttraining's binary_logloss: 0.260756\tvalid_1's binary_logloss: 0.289205\n",
      "[2550]\ttraining's binary_logloss: 0.260556\tvalid_1's binary_logloss: 0.289201\n",
      "[2600]\ttraining's binary_logloss: 0.260334\tvalid_1's binary_logloss: 0.289185\n",
      "[2650]\ttraining's binary_logloss: 0.260068\tvalid_1's binary_logloss: 0.289165\n",
      "[2700]\ttraining's binary_logloss: 0.259709\tvalid_1's binary_logloss: 0.28916\n",
      "[2750]\ttraining's binary_logloss: 0.259517\tvalid_1's binary_logloss: 0.289158\n",
      "[2800]\ttraining's binary_logloss: 0.25924\tvalid_1's binary_logloss: 0.289155\n",
      "[2850]\ttraining's binary_logloss: 0.258977\tvalid_1's binary_logloss: 0.289145\n",
      "[2900]\ttraining's binary_logloss: 0.258782\tvalid_1's binary_logloss: 0.289143\n",
      "[2950]\ttraining's binary_logloss: 0.258615\tvalid_1's binary_logloss: 0.289137\n",
      "[3000]\ttraining's binary_logloss: 0.258422\tvalid_1's binary_logloss: 0.289135\n",
      "[3050]\ttraining's binary_logloss: 0.258174\tvalid_1's binary_logloss: 0.289128\n",
      "[3100]\ttraining's binary_logloss: 0.258051\tvalid_1's binary_logloss: 0.289128\n",
      "[3150]\ttraining's binary_logloss: 0.257888\tvalid_1's binary_logloss: 0.28912\n",
      "[3200]\ttraining's binary_logloss: 0.257676\tvalid_1's binary_logloss: 0.289106\n",
      "[3250]\ttraining's binary_logloss: 0.257408\tvalid_1's binary_logloss: 0.289095\n",
      "[3300]\ttraining's binary_logloss: 0.257206\tvalid_1's binary_logloss: 0.289088\n",
      "[3350]\ttraining's binary_logloss: 0.257074\tvalid_1's binary_logloss: 0.289087\n",
      "[3400]\ttraining's binary_logloss: 0.256978\tvalid_1's binary_logloss: 0.289081\n",
      "[3450]\ttraining's binary_logloss: 0.25671\tvalid_1's binary_logloss: 0.289073\n",
      "[3500]\ttraining's binary_logloss: 0.256516\tvalid_1's binary_logloss: 0.289053\n",
      "[3550]\ttraining's binary_logloss: 0.256348\tvalid_1's binary_logloss: 0.289028\n",
      "[3600]\ttraining's binary_logloss: 0.256142\tvalid_1's binary_logloss: 0.289027\n",
      "[3650]\ttraining's binary_logloss: 0.255973\tvalid_1's binary_logloss: 0.289021\n",
      "[3700]\ttraining's binary_logloss: 0.255732\tvalid_1's binary_logloss: 0.28901\n",
      "[3750]\ttraining's binary_logloss: 0.255602\tvalid_1's binary_logloss: 0.28901\n",
      "[3800]\ttraining's binary_logloss: 0.255549\tvalid_1's binary_logloss: 0.289005\n",
      "[3850]\ttraining's binary_logloss: 0.255401\tvalid_1's binary_logloss: 0.289003\n",
      "[3900]\ttraining's binary_logloss: 0.255253\tvalid_1's binary_logloss: 0.288998\n",
      "[3950]\ttraining's binary_logloss: 0.255097\tvalid_1's binary_logloss: 0.288975\n",
      "[4000]\ttraining's binary_logloss: 0.254905\tvalid_1's binary_logloss: 0.288964\n",
      "[4050]\ttraining's binary_logloss: 0.254749\tvalid_1's binary_logloss: 0.288958\n",
      "[4100]\ttraining's binary_logloss: 0.254686\tvalid_1's binary_logloss: 0.288959\n",
      "[4150]\ttraining's binary_logloss: 0.254567\tvalid_1's binary_logloss: 0.288952\n",
      "[4200]\ttraining's binary_logloss: 0.25444\tvalid_1's binary_logloss: 0.288954\n",
      "[4250]\ttraining's binary_logloss: 0.254286\tvalid_1's binary_logloss: 0.288949\n",
      "[4300]\ttraining's binary_logloss: 0.254176\tvalid_1's binary_logloss: 0.288937\n",
      "[4350]\ttraining's binary_logloss: 0.254045\tvalid_1's binary_logloss: 0.28894\n",
      "[4400]\ttraining's binary_logloss: 0.25397\tvalid_1's binary_logloss: 0.288938\n",
      "[4450]\ttraining's binary_logloss: 0.25382\tvalid_1's binary_logloss: 0.288936\n",
      "[4500]\ttraining's binary_logloss: 0.253725\tvalid_1's binary_logloss: 0.288938\n",
      "[4550]\ttraining's binary_logloss: 0.25362\tvalid_1's binary_logloss: 0.288938\n",
      "[4600]\ttraining's binary_logloss: 0.253492\tvalid_1's binary_logloss: 0.288939\n",
      "[4650]\ttraining's binary_logloss: 0.253339\tvalid_1's binary_logloss: 0.288927\n",
      "[4700]\ttraining's binary_logloss: 0.253255\tvalid_1's binary_logloss: 0.288928\n",
      "[4750]\ttraining's binary_logloss: 0.25319\tvalid_1's binary_logloss: 0.288924\n",
      "[4800]\ttraining's binary_logloss: 0.25311\tvalid_1's binary_logloss: 0.288924\n",
      "[4850]\ttraining's binary_logloss: 0.253032\tvalid_1's binary_logloss: 0.288925\n",
      "[4900]\ttraining's binary_logloss: 0.252986\tvalid_1's binary_logloss: 0.288927\n",
      "[4950]\ttraining's binary_logloss: 0.252852\tvalid_1's binary_logloss: 0.288918\n",
      "[5000]\ttraining's binary_logloss: 0.25277\tvalid_1's binary_logloss: 0.288917\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.25277\tvalid_1's binary_logloss: 0.288917\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.329668\tvalid_1's binary_logloss: 0.329785\n",
      "[100]\ttraining's binary_logloss: 0.316939\tvalid_1's binary_logloss: 0.31741\n",
      "[150]\ttraining's binary_logloss: 0.311117\tvalid_1's binary_logloss: 0.311934\n",
      "[200]\ttraining's binary_logloss: 0.306961\tvalid_1's binary_logloss: 0.308136\n",
      "[250]\ttraining's binary_logloss: 0.304397\tvalid_1's binary_logloss: 0.305916\n",
      "[300]\ttraining's binary_logloss: 0.301836\tvalid_1's binary_logloss: 0.303684\n",
      "[350]\ttraining's binary_logloss: 0.299962\tvalid_1's binary_logloss: 0.302098\n",
      "[400]\ttraining's binary_logloss: 0.298453\tvalid_1's binary_logloss: 0.300894\n",
      "[450]\ttraining's binary_logloss: 0.296877\tvalid_1's binary_logloss: 0.29959\n",
      "[500]\ttraining's binary_logloss: 0.295813\tvalid_1's binary_logloss: 0.298853\n",
      "[550]\ttraining's binary_logloss: 0.294816\tvalid_1's binary_logloss: 0.298145\n",
      "[600]\ttraining's binary_logloss: 0.293684\tvalid_1's binary_logloss: 0.297287\n",
      "[650]\ttraining's binary_logloss: 0.292538\tvalid_1's binary_logloss: 0.29647\n",
      "[700]\ttraining's binary_logloss: 0.291571\tvalid_1's binary_logloss: 0.295796\n",
      "[750]\ttraining's binary_logloss: 0.290796\tvalid_1's binary_logloss: 0.295359\n",
      "[800]\ttraining's binary_logloss: 0.290077\tvalid_1's binary_logloss: 0.294938\n",
      "[850]\ttraining's binary_logloss: 0.289425\tvalid_1's binary_logloss: 0.294614\n",
      "[900]\ttraining's binary_logloss: 0.288856\tvalid_1's binary_logloss: 0.294311\n",
      "[950]\ttraining's binary_logloss: 0.288272\tvalid_1's binary_logloss: 0.294001\n",
      "[1000]\ttraining's binary_logloss: 0.28772\tvalid_1's binary_logloss: 0.293753\n",
      "[1050]\ttraining's binary_logloss: 0.287268\tvalid_1's binary_logloss: 0.293613\n",
      "[1100]\ttraining's binary_logloss: 0.286855\tvalid_1's binary_logloss: 0.293485\n",
      "[1150]\ttraining's binary_logloss: 0.286422\tvalid_1's binary_logloss: 0.293333\n",
      "[1200]\ttraining's binary_logloss: 0.286015\tvalid_1's binary_logloss: 0.293215\n",
      "[1250]\ttraining's binary_logloss: 0.28561\tvalid_1's binary_logloss: 0.293077\n",
      "[1300]\ttraining's binary_logloss: 0.285213\tvalid_1's binary_logloss: 0.292962\n",
      "[1350]\ttraining's binary_logloss: 0.284796\tvalid_1's binary_logloss: 0.292854\n",
      "[1400]\ttraining's binary_logloss: 0.284401\tvalid_1's binary_logloss: 0.292732\n",
      "[1450]\ttraining's binary_logloss: 0.284042\tvalid_1's binary_logloss: 0.292653\n",
      "[1500]\ttraining's binary_logloss: 0.283633\tvalid_1's binary_logloss: 0.292508\n",
      "[1550]\ttraining's binary_logloss: 0.283194\tvalid_1's binary_logloss: 0.292411\n",
      "[1600]\ttraining's binary_logloss: 0.282755\tvalid_1's binary_logloss: 0.292238\n",
      "[1650]\ttraining's binary_logloss: 0.282393\tvalid_1's binary_logloss: 0.292163\n",
      "[1700]\ttraining's binary_logloss: 0.282019\tvalid_1's binary_logloss: 0.292092\n",
      "[1750]\ttraining's binary_logloss: 0.281663\tvalid_1's binary_logloss: 0.292018\n",
      "[1800]\ttraining's binary_logloss: 0.281287\tvalid_1's binary_logloss: 0.29193\n",
      "[1850]\ttraining's binary_logloss: 0.280914\tvalid_1's binary_logloss: 0.291861\n",
      "[1900]\ttraining's binary_logloss: 0.280594\tvalid_1's binary_logloss: 0.291811\n",
      "[1950]\ttraining's binary_logloss: 0.280252\tvalid_1's binary_logloss: 0.291742\n",
      "[2000]\ttraining's binary_logloss: 0.279922\tvalid_1's binary_logloss: 0.291679\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's binary_logloss: 0.279922\tvalid_1's binary_logloss: 0.291679\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.328953\tvalid_1's binary_logloss: 0.328879\n",
      "[100]\ttraining's binary_logloss: 0.316481\tvalid_1's binary_logloss: 0.31666\n",
      "[150]\ttraining's binary_logloss: 0.310346\tvalid_1's binary_logloss: 0.31083\n",
      "[200]\ttraining's binary_logloss: 0.306883\tvalid_1's binary_logloss: 0.307746\n",
      "[250]\ttraining's binary_logloss: 0.304069\tvalid_1's binary_logloss: 0.305271\n",
      "[300]\ttraining's binary_logloss: 0.302068\tvalid_1's binary_logloss: 0.303541\n",
      "[350]\ttraining's binary_logloss: 0.300198\tvalid_1's binary_logloss: 0.301963\n",
      "[400]\ttraining's binary_logloss: 0.298774\tvalid_1's binary_logloss: 0.300861\n",
      "[450]\ttraining's binary_logloss: 0.297009\tvalid_1's binary_logloss: 0.299403\n",
      "[500]\ttraining's binary_logloss: 0.295651\tvalid_1's binary_logloss: 0.298338\n",
      "[550]\ttraining's binary_logloss: 0.294713\tvalid_1's binary_logloss: 0.297687\n",
      "[600]\ttraining's binary_logloss: 0.293588\tvalid_1's binary_logloss: 0.296906\n",
      "[650]\ttraining's binary_logloss: 0.292706\tvalid_1's binary_logloss: 0.296349\n",
      "[700]\ttraining's binary_logloss: 0.291619\tvalid_1's binary_logloss: 0.29548\n",
      "[750]\ttraining's binary_logloss: 0.291052\tvalid_1's binary_logloss: 0.295244\n",
      "[800]\ttraining's binary_logloss: 0.290336\tvalid_1's binary_logloss: 0.294868\n",
      "[850]\ttraining's binary_logloss: 0.28968\tvalid_1's binary_logloss: 0.294514\n",
      "[900]\ttraining's binary_logloss: 0.289149\tvalid_1's binary_logloss: 0.29426\n",
      "[950]\ttraining's binary_logloss: 0.288399\tvalid_1's binary_logloss: 0.293822\n",
      "[1000]\ttraining's binary_logloss: 0.287971\tvalid_1's binary_logloss: 0.293612\n",
      "[1050]\ttraining's binary_logloss: 0.287489\tvalid_1's binary_logloss: 0.293389\n",
      "[1100]\ttraining's binary_logloss: 0.286809\tvalid_1's binary_logloss: 0.292983\n",
      "[1150]\ttraining's binary_logloss: 0.286441\tvalid_1's binary_logloss: 0.292876\n",
      "[1200]\ttraining's binary_logloss: 0.285959\tvalid_1's binary_logloss: 0.292703\n",
      "[1250]\ttraining's binary_logloss: 0.285577\tvalid_1's binary_logloss: 0.292592\n",
      "[1300]\ttraining's binary_logloss: 0.285201\tvalid_1's binary_logloss: 0.292484\n",
      "[1350]\ttraining's binary_logloss: 0.284768\tvalid_1's binary_logloss: 0.292355\n",
      "[1400]\ttraining's binary_logloss: 0.284427\tvalid_1's binary_logloss: 0.29229\n",
      "[1450]\ttraining's binary_logloss: 0.284042\tvalid_1's binary_logloss: 0.292197\n",
      "[1500]\ttraining's binary_logloss: 0.283762\tvalid_1's binary_logloss: 0.292143\n",
      "[1550]\ttraining's binary_logloss: 0.28341\tvalid_1's binary_logloss: 0.292072\n",
      "[1600]\ttraining's binary_logloss: 0.283022\tvalid_1's binary_logloss: 0.291993\n",
      "[1650]\ttraining's binary_logloss: 0.282644\tvalid_1's binary_logloss: 0.291892\n",
      "[1700]\ttraining's binary_logloss: 0.282282\tvalid_1's binary_logloss: 0.291796\n",
      "[1750]\ttraining's binary_logloss: 0.281884\tvalid_1's binary_logloss: 0.291698\n",
      "[1800]\ttraining's binary_logloss: 0.28149\tvalid_1's binary_logloss: 0.291592\n",
      "[1850]\ttraining's binary_logloss: 0.281115\tvalid_1's binary_logloss: 0.291478\n",
      "[1900]\ttraining's binary_logloss: 0.280598\tvalid_1's binary_logloss: 0.291274\n",
      "[1950]\ttraining's binary_logloss: 0.280202\tvalid_1's binary_logloss: 0.291174\n",
      "[2000]\ttraining's binary_logloss: 0.279783\tvalid_1's binary_logloss: 0.291047\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's binary_logloss: 0.279783\tvalid_1's binary_logloss: 0.291047\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.329418\tvalid_1's binary_logloss: 0.329764\n",
      "[100]\ttraining's binary_logloss: 0.316473\tvalid_1's binary_logloss: 0.317275\n",
      "[150]\ttraining's binary_logloss: 0.310472\tvalid_1's binary_logloss: 0.311677\n",
      "[200]\ttraining's binary_logloss: 0.306817\tvalid_1's binary_logloss: 0.308438\n",
      "[250]\ttraining's binary_logloss: 0.304024\tvalid_1's binary_logloss: 0.306002\n",
      "[300]\ttraining's binary_logloss: 0.301987\tvalid_1's binary_logloss: 0.304322\n",
      "[350]\ttraining's binary_logloss: 0.300142\tvalid_1's binary_logloss: 0.302805\n",
      "[400]\ttraining's binary_logloss: 0.298347\tvalid_1's binary_logloss: 0.301306\n",
      "[450]\ttraining's binary_logloss: 0.296951\tvalid_1's binary_logloss: 0.300271\n",
      "[500]\ttraining's binary_logloss: 0.295751\tvalid_1's binary_logloss: 0.299364\n",
      "[550]\ttraining's binary_logloss: 0.294851\tvalid_1's binary_logloss: 0.298745\n",
      "[600]\ttraining's binary_logloss: 0.293787\tvalid_1's binary_logloss: 0.297984\n",
      "[650]\ttraining's binary_logloss: 0.292664\tvalid_1's binary_logloss: 0.297121\n",
      "[700]\ttraining's binary_logloss: 0.291753\tvalid_1's binary_logloss: 0.296494\n",
      "[750]\ttraining's binary_logloss: 0.29109\tvalid_1's binary_logloss: 0.296175\n",
      "[800]\ttraining's binary_logloss: 0.29019\tvalid_1's binary_logloss: 0.295569\n",
      "[850]\ttraining's binary_logloss: 0.289633\tvalid_1's binary_logloss: 0.295316\n",
      "[900]\ttraining's binary_logloss: 0.28886\tvalid_1's binary_logloss: 0.294842\n",
      "[950]\ttraining's binary_logloss: 0.288369\tvalid_1's binary_logloss: 0.294625\n",
      "[1000]\ttraining's binary_logloss: 0.287854\tvalid_1's binary_logloss: 0.294394\n",
      "[1050]\ttraining's binary_logloss: 0.287391\tvalid_1's binary_logloss: 0.29423\n",
      "[1100]\ttraining's binary_logloss: 0.286981\tvalid_1's binary_logloss: 0.294134\n",
      "[1150]\ttraining's binary_logloss: 0.286448\tvalid_1's binary_logloss: 0.293899\n",
      "[1200]\ttraining's binary_logloss: 0.286008\tvalid_1's binary_logloss: 0.293736\n",
      "[1250]\ttraining's binary_logloss: 0.28555\tvalid_1's binary_logloss: 0.293558\n",
      "[1300]\ttraining's binary_logloss: 0.285103\tvalid_1's binary_logloss: 0.293424\n",
      "[1350]\ttraining's binary_logloss: 0.284691\tvalid_1's binary_logloss: 0.293325\n",
      "[1400]\ttraining's binary_logloss: 0.284262\tvalid_1's binary_logloss: 0.293205\n",
      "[1450]\ttraining's binary_logloss: 0.283801\tvalid_1's binary_logloss: 0.293067\n",
      "[1500]\ttraining's binary_logloss: 0.283375\tvalid_1's binary_logloss: 0.292935\n",
      "[1550]\ttraining's binary_logloss: 0.283035\tvalid_1's binary_logloss: 0.292866\n",
      "[1600]\ttraining's binary_logloss: 0.282584\tvalid_1's binary_logloss: 0.292691\n",
      "[1650]\ttraining's binary_logloss: 0.282192\tvalid_1's binary_logloss: 0.292584\n",
      "[1700]\ttraining's binary_logloss: 0.281844\tvalid_1's binary_logloss: 0.292519\n",
      "[1750]\ttraining's binary_logloss: 0.281427\tvalid_1's binary_logloss: 0.29241\n",
      "[1800]\ttraining's binary_logloss: 0.281071\tvalid_1's binary_logloss: 0.292333\n",
      "[1850]\ttraining's binary_logloss: 0.280707\tvalid_1's binary_logloss: 0.292255\n",
      "[1900]\ttraining's binary_logloss: 0.280368\tvalid_1's binary_logloss: 0.292198\n",
      "[1950]\ttraining's binary_logloss: 0.279993\tvalid_1's binary_logloss: 0.292136\n",
      "[2000]\ttraining's binary_logloss: 0.279608\tvalid_1's binary_logloss: 0.292038\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's binary_logloss: 0.279608\tvalid_1's binary_logloss: 0.292038\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.329897\tvalid_1's binary_logloss: 0.330282\n",
      "[100]\ttraining's binary_logloss: 0.31721\tvalid_1's binary_logloss: 0.31781\n",
      "[150]\ttraining's binary_logloss: 0.310612\tvalid_1's binary_logloss: 0.311487\n",
      "[200]\ttraining's binary_logloss: 0.307243\tvalid_1's binary_logloss: 0.30845\n",
      "[250]\ttraining's binary_logloss: 0.303691\tvalid_1's binary_logloss: 0.305234\n",
      "[300]\ttraining's binary_logloss: 0.301382\tvalid_1's binary_logloss: 0.303221\n",
      "[350]\ttraining's binary_logloss: 0.299741\tvalid_1's binary_logloss: 0.30187\n",
      "[400]\ttraining's binary_logloss: 0.298164\tvalid_1's binary_logloss: 0.300581\n",
      "[450]\ttraining's binary_logloss: 0.297031\tvalid_1's binary_logloss: 0.299738\n",
      "[500]\ttraining's binary_logloss: 0.295463\tvalid_1's binary_logloss: 0.298464\n",
      "[550]\ttraining's binary_logloss: 0.294383\tvalid_1's binary_logloss: 0.297689\n",
      "[600]\ttraining's binary_logloss: 0.29359\tvalid_1's binary_logloss: 0.297175\n",
      "[650]\ttraining's binary_logloss: 0.292798\tvalid_1's binary_logloss: 0.296701\n",
      "[700]\ttraining's binary_logloss: 0.29181\tvalid_1's binary_logloss: 0.296008\n",
      "[750]\ttraining's binary_logloss: 0.291134\tvalid_1's binary_logloss: 0.295578\n",
      "[800]\ttraining's binary_logloss: 0.290516\tvalid_1's binary_logloss: 0.295288\n",
      "[850]\ttraining's binary_logloss: 0.289884\tvalid_1's binary_logloss: 0.294954\n",
      "[900]\ttraining's binary_logloss: 0.289212\tvalid_1's binary_logloss: 0.2946\n",
      "[950]\ttraining's binary_logloss: 0.288646\tvalid_1's binary_logloss: 0.294294\n",
      "[1000]\ttraining's binary_logloss: 0.288109\tvalid_1's binary_logloss: 0.294068\n",
      "[1050]\ttraining's binary_logloss: 0.287492\tvalid_1's binary_logloss: 0.29376\n",
      "[1100]\ttraining's binary_logloss: 0.287115\tvalid_1's binary_logloss: 0.293684\n",
      "[1150]\ttraining's binary_logloss: 0.286535\tvalid_1's binary_logloss: 0.293423\n",
      "[1200]\ttraining's binary_logloss: 0.286159\tvalid_1's binary_logloss: 0.293315\n",
      "[1250]\ttraining's binary_logloss: 0.28566\tvalid_1's binary_logloss: 0.293174\n",
      "[1300]\ttraining's binary_logloss: 0.285192\tvalid_1's binary_logloss: 0.293009\n",
      "[1350]\ttraining's binary_logloss: 0.284655\tvalid_1's binary_logloss: 0.29278\n",
      "[1400]\ttraining's binary_logloss: 0.284249\tvalid_1's binary_logloss: 0.292661\n",
      "[1450]\ttraining's binary_logloss: 0.283797\tvalid_1's binary_logloss: 0.292489\n",
      "[1500]\ttraining's binary_logloss: 0.283477\tvalid_1's binary_logloss: 0.29244\n",
      "[1550]\ttraining's binary_logloss: 0.283064\tvalid_1's binary_logloss: 0.292358\n",
      "[1600]\ttraining's binary_logloss: 0.282619\tvalid_1's binary_logloss: 0.292202\n",
      "[1650]\ttraining's binary_logloss: 0.282268\tvalid_1's binary_logloss: 0.292146\n",
      "[1700]\ttraining's binary_logloss: 0.281906\tvalid_1's binary_logloss: 0.292078\n",
      "[1750]\ttraining's binary_logloss: 0.281551\tvalid_1's binary_logloss: 0.291993\n",
      "[1800]\ttraining's binary_logloss: 0.281183\tvalid_1's binary_logloss: 0.291901\n",
      "[1850]\ttraining's binary_logloss: 0.280863\tvalid_1's binary_logloss: 0.291865\n",
      "[1900]\ttraining's binary_logloss: 0.280535\tvalid_1's binary_logloss: 0.291842\n",
      "[1950]\ttraining's binary_logloss: 0.280153\tvalid_1's binary_logloss: 0.291741\n",
      "[2000]\ttraining's binary_logloss: 0.279835\tvalid_1's binary_logloss: 0.291697\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's binary_logloss: 0.279835\tvalid_1's binary_logloss: 0.291697\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.329241\tvalid_1's binary_logloss: 0.329777\n",
      "[100]\ttraining's binary_logloss: 0.316367\tvalid_1's binary_logloss: 0.317384\n",
      "[150]\ttraining's binary_logloss: 0.30981\tvalid_1's binary_logloss: 0.311197\n",
      "[200]\ttraining's binary_logloss: 0.306347\tvalid_1's binary_logloss: 0.308122\n",
      "[250]\ttraining's binary_logloss: 0.303679\tvalid_1's binary_logloss: 0.305825\n",
      "[300]\ttraining's binary_logloss: 0.301562\tvalid_1's binary_logloss: 0.30402\n",
      "[350]\ttraining's binary_logloss: 0.299673\tvalid_1's binary_logloss: 0.302492\n",
      "[400]\ttraining's binary_logloss: 0.298239\tvalid_1's binary_logloss: 0.301411\n",
      "[450]\ttraining's binary_logloss: 0.296735\tvalid_1's binary_logloss: 0.3002\n",
      "[500]\ttraining's binary_logloss: 0.29561\tvalid_1's binary_logloss: 0.299366\n",
      "[550]\ttraining's binary_logloss: 0.294577\tvalid_1's binary_logloss: 0.298624\n",
      "[600]\ttraining's binary_logloss: 0.293711\tvalid_1's binary_logloss: 0.298089\n",
      "[650]\ttraining's binary_logloss: 0.292803\tvalid_1's binary_logloss: 0.297444\n",
      "[700]\ttraining's binary_logloss: 0.291736\tvalid_1's binary_logloss: 0.296675\n",
      "[750]\ttraining's binary_logloss: 0.291132\tvalid_1's binary_logloss: 0.296413\n",
      "[800]\ttraining's binary_logloss: 0.290486\tvalid_1's binary_logloss: 0.296072\n",
      "[850]\ttraining's binary_logloss: 0.289826\tvalid_1's binary_logloss: 0.29571\n",
      "[900]\ttraining's binary_logloss: 0.289328\tvalid_1's binary_logloss: 0.295509\n",
      "[950]\ttraining's binary_logloss: 0.288817\tvalid_1's binary_logloss: 0.295329\n",
      "[1000]\ttraining's binary_logloss: 0.288254\tvalid_1's binary_logloss: 0.295065\n",
      "[1050]\ttraining's binary_logloss: 0.287665\tvalid_1's binary_logloss: 0.29477\n",
      "[1100]\ttraining's binary_logloss: 0.287214\tvalid_1's binary_logloss: 0.294584\n",
      "[1150]\ttraining's binary_logloss: 0.286595\tvalid_1's binary_logloss: 0.294244\n",
      "[1200]\ttraining's binary_logloss: 0.286184\tvalid_1's binary_logloss: 0.294129\n",
      "[1250]\ttraining's binary_logloss: 0.285726\tvalid_1's binary_logloss: 0.293957\n",
      "[1300]\ttraining's binary_logloss: 0.285322\tvalid_1's binary_logloss: 0.293827\n",
      "[1350]\ttraining's binary_logloss: 0.284951\tvalid_1's binary_logloss: 0.293755\n",
      "[1400]\ttraining's binary_logloss: 0.284496\tvalid_1's binary_logloss: 0.2936\n",
      "[1450]\ttraining's binary_logloss: 0.284085\tvalid_1's binary_logloss: 0.293521\n",
      "[1500]\ttraining's binary_logloss: 0.283738\tvalid_1's binary_logloss: 0.293447\n",
      "[1550]\ttraining's binary_logloss: 0.283396\tvalid_1's binary_logloss: 0.293385\n",
      "[1600]\ttraining's binary_logloss: 0.282968\tvalid_1's binary_logloss: 0.293249\n",
      "[1650]\ttraining's binary_logloss: 0.282459\tvalid_1's binary_logloss: 0.293028\n",
      "[1700]\ttraining's binary_logloss: 0.282071\tvalid_1's binary_logloss: 0.292938\n",
      "[1750]\ttraining's binary_logloss: 0.281703\tvalid_1's binary_logloss: 0.292829\n",
      "[1800]\ttraining's binary_logloss: 0.281384\tvalid_1's binary_logloss: 0.292765\n",
      "[1850]\ttraining's binary_logloss: 0.281008\tvalid_1's binary_logloss: 0.292664\n",
      "[1900]\ttraining's binary_logloss: 0.280634\tvalid_1's binary_logloss: 0.292582\n",
      "[1950]\ttraining's binary_logloss: 0.280236\tvalid_1's binary_logloss: 0.29249\n",
      "[2000]\ttraining's binary_logloss: 0.279891\tvalid_1's binary_logloss: 0.292458\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's binary_logloss: 0.279891\tvalid_1's binary_logloss: 0.292458\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.358095\tvalid_1's binary_logloss: 0.358018\n",
      "[100]\ttraining's binary_logloss: 0.340253\tvalid_1's binary_logloss: 0.340227\n",
      "[150]\ttraining's binary_logloss: 0.331739\tvalid_1's binary_logloss: 0.331801\n",
      "[200]\ttraining's binary_logloss: 0.325768\tvalid_1's binary_logloss: 0.325918\n",
      "[250]\ttraining's binary_logloss: 0.321558\tvalid_1's binary_logloss: 0.32182\n",
      "[300]\ttraining's binary_logloss: 0.318793\tvalid_1's binary_logloss: 0.319141\n",
      "[350]\ttraining's binary_logloss: 0.316659\tvalid_1's binary_logloss: 0.317078\n",
      "[400]\ttraining's binary_logloss: 0.31488\tvalid_1's binary_logloss: 0.315347\n",
      "[450]\ttraining's binary_logloss: 0.313465\tvalid_1's binary_logloss: 0.314014\n",
      "[500]\ttraining's binary_logloss: 0.31201\tvalid_1's binary_logloss: 0.31265\n",
      "[550]\ttraining's binary_logloss: 0.310793\tvalid_1's binary_logloss: 0.311507\n",
      "[600]\ttraining's binary_logloss: 0.309695\tvalid_1's binary_logloss: 0.31049\n",
      "[650]\ttraining's binary_logloss: 0.308532\tvalid_1's binary_logloss: 0.309398\n",
      "[700]\ttraining's binary_logloss: 0.307688\tvalid_1's binary_logloss: 0.308618\n",
      "[750]\ttraining's binary_logloss: 0.306715\tvalid_1's binary_logloss: 0.307725\n",
      "[800]\ttraining's binary_logloss: 0.306022\tvalid_1's binary_logloss: 0.307104\n",
      "[850]\ttraining's binary_logloss: 0.305543\tvalid_1's binary_logloss: 0.306698\n",
      "[900]\ttraining's binary_logloss: 0.304836\tvalid_1's binary_logloss: 0.30606\n",
      "[950]\ttraining's binary_logloss: 0.304287\tvalid_1's binary_logloss: 0.305592\n",
      "[1000]\ttraining's binary_logloss: 0.303658\tvalid_1's binary_logloss: 0.305031\n",
      "[1050]\ttraining's binary_logloss: 0.303062\tvalid_1's binary_logloss: 0.304483\n",
      "[1100]\ttraining's binary_logloss: 0.302437\tvalid_1's binary_logloss: 0.303904\n",
      "[1150]\ttraining's binary_logloss: 0.302052\tvalid_1's binary_logloss: 0.303589\n",
      "[1200]\ttraining's binary_logloss: 0.301563\tvalid_1's binary_logloss: 0.303162\n",
      "[1250]\ttraining's binary_logloss: 0.301071\tvalid_1's binary_logloss: 0.302729\n",
      "[1300]\ttraining's binary_logloss: 0.300692\tvalid_1's binary_logloss: 0.302401\n",
      "[1350]\ttraining's binary_logloss: 0.30036\tvalid_1's binary_logloss: 0.302114\n",
      "[1400]\ttraining's binary_logloss: 0.299927\tvalid_1's binary_logloss: 0.301745\n",
      "[1450]\ttraining's binary_logloss: 0.299536\tvalid_1's binary_logloss: 0.30141\n",
      "[1500]\ttraining's binary_logloss: 0.29921\tvalid_1's binary_logloss: 0.301144\n",
      "[1550]\ttraining's binary_logloss: 0.298807\tvalid_1's binary_logloss: 0.300814\n",
      "[1600]\ttraining's binary_logloss: 0.29847\tvalid_1's binary_logloss: 0.30053\n",
      "[1650]\ttraining's binary_logloss: 0.298221\tvalid_1's binary_logloss: 0.300333\n",
      "[1700]\ttraining's binary_logloss: 0.297893\tvalid_1's binary_logloss: 0.300056\n",
      "[1750]\ttraining's binary_logloss: 0.2974\tvalid_1's binary_logloss: 0.29963\n",
      "[1800]\ttraining's binary_logloss: 0.297021\tvalid_1's binary_logloss: 0.29931\n",
      "[1850]\ttraining's binary_logloss: 0.296755\tvalid_1's binary_logloss: 0.299098\n",
      "[1900]\ttraining's binary_logloss: 0.296536\tvalid_1's binary_logloss: 0.298931\n",
      "[1950]\ttraining's binary_logloss: 0.296343\tvalid_1's binary_logloss: 0.298804\n",
      "[2000]\ttraining's binary_logloss: 0.296056\tvalid_1's binary_logloss: 0.298582\n",
      "[2050]\ttraining's binary_logloss: 0.295829\tvalid_1's binary_logloss: 0.298412\n",
      "[2100]\ttraining's binary_logloss: 0.295644\tvalid_1's binary_logloss: 0.298288\n",
      "[2150]\ttraining's binary_logloss: 0.295447\tvalid_1's binary_logloss: 0.298149\n",
      "[2200]\ttraining's binary_logloss: 0.295176\tvalid_1's binary_logloss: 0.297956\n",
      "[2250]\ttraining's binary_logloss: 0.295009\tvalid_1's binary_logloss: 0.297835\n",
      "[2300]\ttraining's binary_logloss: 0.294764\tvalid_1's binary_logloss: 0.297649\n",
      "[2350]\ttraining's binary_logloss: 0.294586\tvalid_1's binary_logloss: 0.297535\n",
      "[2400]\ttraining's binary_logloss: 0.29435\tvalid_1's binary_logloss: 0.297367\n",
      "[2450]\ttraining's binary_logloss: 0.294164\tvalid_1's binary_logloss: 0.297242\n",
      "[2500]\ttraining's binary_logloss: 0.293948\tvalid_1's binary_logloss: 0.297095\n",
      "[2550]\ttraining's binary_logloss: 0.293759\tvalid_1's binary_logloss: 0.296965\n",
      "[2600]\ttraining's binary_logloss: 0.293539\tvalid_1's binary_logloss: 0.29681\n",
      "[2650]\ttraining's binary_logloss: 0.293355\tvalid_1's binary_logloss: 0.296684\n",
      "[2700]\ttraining's binary_logloss: 0.293097\tvalid_1's binary_logloss: 0.296489\n",
      "[2750]\ttraining's binary_logloss: 0.292931\tvalid_1's binary_logloss: 0.296388\n",
      "[2800]\ttraining's binary_logloss: 0.292797\tvalid_1's binary_logloss: 0.296309\n",
      "[2850]\ttraining's binary_logloss: 0.292666\tvalid_1's binary_logloss: 0.296236\n",
      "[2900]\ttraining's binary_logloss: 0.292454\tvalid_1's binary_logloss: 0.296083\n",
      "[2950]\ttraining's binary_logloss: 0.292324\tvalid_1's binary_logloss: 0.296007\n",
      "[3000]\ttraining's binary_logloss: 0.2922\tvalid_1's binary_logloss: 0.295946\n",
      "[3050]\ttraining's binary_logloss: 0.292033\tvalid_1's binary_logloss: 0.295862\n",
      "[3100]\ttraining's binary_logloss: 0.291909\tvalid_1's binary_logloss: 0.295799\n",
      "[3150]\ttraining's binary_logloss: 0.291768\tvalid_1's binary_logloss: 0.29573\n",
      "[3200]\ttraining's binary_logloss: 0.29149\tvalid_1's binary_logloss: 0.295523\n",
      "[3250]\ttraining's binary_logloss: 0.291326\tvalid_1's binary_logloss: 0.295412\n",
      "[3300]\ttraining's binary_logloss: 0.291175\tvalid_1's binary_logloss: 0.295311\n",
      "[3350]\ttraining's binary_logloss: 0.291051\tvalid_1's binary_logloss: 0.295246\n",
      "[3400]\ttraining's binary_logloss: 0.290875\tvalid_1's binary_logloss: 0.295125\n",
      "[3450]\ttraining's binary_logloss: 0.290771\tvalid_1's binary_logloss: 0.295091\n",
      "[3500]\ttraining's binary_logloss: 0.290648\tvalid_1's binary_logloss: 0.295044\n",
      "[3550]\ttraining's binary_logloss: 0.290548\tvalid_1's binary_logloss: 0.295006\n",
      "[3600]\ttraining's binary_logloss: 0.290433\tvalid_1's binary_logloss: 0.294956\n",
      "[3650]\ttraining's binary_logloss: 0.290328\tvalid_1's binary_logloss: 0.2949\n",
      "[3700]\ttraining's binary_logloss: 0.290214\tvalid_1's binary_logloss: 0.294853\n",
      "[3750]\ttraining's binary_logloss: 0.290066\tvalid_1's binary_logloss: 0.294766\n",
      "[3800]\ttraining's binary_logloss: 0.289947\tvalid_1's binary_logloss: 0.294714\n",
      "[3850]\ttraining's binary_logloss: 0.28983\tvalid_1's binary_logloss: 0.294644\n",
      "[3900]\ttraining's binary_logloss: 0.289728\tvalid_1's binary_logloss: 0.294597\n",
      "[3950]\ttraining's binary_logloss: 0.289505\tvalid_1's binary_logloss: 0.294423\n",
      "[4000]\ttraining's binary_logloss: 0.289403\tvalid_1's binary_logloss: 0.294377\n",
      "[4050]\ttraining's binary_logloss: 0.289281\tvalid_1's binary_logloss: 0.294331\n",
      "[4100]\ttraining's binary_logloss: 0.289179\tvalid_1's binary_logloss: 0.294296\n",
      "[4150]\ttraining's binary_logloss: 0.289037\tvalid_1's binary_logloss: 0.294223\n",
      "[4200]\ttraining's binary_logloss: 0.288898\tvalid_1's binary_logloss: 0.294147\n",
      "[4250]\ttraining's binary_logloss: 0.288794\tvalid_1's binary_logloss: 0.294102\n",
      "[4300]\ttraining's binary_logloss: 0.288698\tvalid_1's binary_logloss: 0.294069\n",
      "[4350]\ttraining's binary_logloss: 0.288567\tvalid_1's binary_logloss: 0.294001\n",
      "[4400]\ttraining's binary_logloss: 0.288431\tvalid_1's binary_logloss: 0.293916\n",
      "[4450]\ttraining's binary_logloss: 0.288335\tvalid_1's binary_logloss: 0.293882\n",
      "[4500]\ttraining's binary_logloss: 0.288248\tvalid_1's binary_logloss: 0.293853\n",
      "[4550]\ttraining's binary_logloss: 0.288145\tvalid_1's binary_logloss: 0.293808\n",
      "[4600]\ttraining's binary_logloss: 0.288059\tvalid_1's binary_logloss: 0.293781\n",
      "[4650]\ttraining's binary_logloss: 0.287953\tvalid_1's binary_logloss: 0.293731\n",
      "[4700]\ttraining's binary_logloss: 0.287876\tvalid_1's binary_logloss: 0.293717\n",
      "[4750]\ttraining's binary_logloss: 0.287781\tvalid_1's binary_logloss: 0.293686\n",
      "[4800]\ttraining's binary_logloss: 0.287667\tvalid_1's binary_logloss: 0.293632\n",
      "[4850]\ttraining's binary_logloss: 0.287551\tvalid_1's binary_logloss: 0.293578\n",
      "[4900]\ttraining's binary_logloss: 0.287457\tvalid_1's binary_logloss: 0.293538\n",
      "[4950]\ttraining's binary_logloss: 0.287341\tvalid_1's binary_logloss: 0.293482\n",
      "[5000]\ttraining's binary_logloss: 0.287215\tvalid_1's binary_logloss: 0.293419\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.287215\tvalid_1's binary_logloss: 0.293419\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.357801\tvalid_1's binary_logloss: 0.357512\n",
      "[100]\ttraining's binary_logloss: 0.339813\tvalid_1's binary_logloss: 0.339552\n",
      "[150]\ttraining's binary_logloss: 0.331645\tvalid_1's binary_logloss: 0.331405\n",
      "[200]\ttraining's binary_logloss: 0.326276\tvalid_1's binary_logloss: 0.326142\n",
      "[250]\ttraining's binary_logloss: 0.322331\tvalid_1's binary_logloss: 0.32228\n",
      "[300]\ttraining's binary_logloss: 0.319349\tvalid_1's binary_logloss: 0.319366\n",
      "[350]\ttraining's binary_logloss: 0.317157\tvalid_1's binary_logloss: 0.317259\n",
      "[400]\ttraining's binary_logloss: 0.315129\tvalid_1's binary_logloss: 0.315285\n",
      "[450]\ttraining's binary_logloss: 0.313421\tvalid_1's binary_logloss: 0.313659\n",
      "[500]\ttraining's binary_logloss: 0.312275\tvalid_1's binary_logloss: 0.312578\n",
      "[550]\ttraining's binary_logloss: 0.311059\tvalid_1's binary_logloss: 0.311432\n",
      "[600]\ttraining's binary_logloss: 0.310099\tvalid_1's binary_logloss: 0.31053\n",
      "[650]\ttraining's binary_logloss: 0.308904\tvalid_1's binary_logloss: 0.309399\n",
      "[700]\ttraining's binary_logloss: 0.307784\tvalid_1's binary_logloss: 0.30833\n",
      "[750]\ttraining's binary_logloss: 0.307122\tvalid_1's binary_logloss: 0.307728\n",
      "[800]\ttraining's binary_logloss: 0.30627\tvalid_1's binary_logloss: 0.306928\n",
      "[850]\ttraining's binary_logloss: 0.305578\tvalid_1's binary_logloss: 0.306301\n",
      "[900]\ttraining's binary_logloss: 0.305061\tvalid_1's binary_logloss: 0.305843\n",
      "[950]\ttraining's binary_logloss: 0.304312\tvalid_1's binary_logloss: 0.305151\n",
      "[1000]\ttraining's binary_logloss: 0.303822\tvalid_1's binary_logloss: 0.304724\n",
      "[1050]\ttraining's binary_logloss: 0.303284\tvalid_1's binary_logloss: 0.30425\n",
      "[1100]\ttraining's binary_logloss: 0.302767\tvalid_1's binary_logloss: 0.303788\n",
      "[1150]\ttraining's binary_logloss: 0.302274\tvalid_1's binary_logloss: 0.303361\n",
      "[1200]\ttraining's binary_logloss: 0.301732\tvalid_1's binary_logloss: 0.302875\n",
      "[1250]\ttraining's binary_logloss: 0.301288\tvalid_1's binary_logloss: 0.302499\n",
      "[1300]\ttraining's binary_logloss: 0.3008\tvalid_1's binary_logloss: 0.302069\n",
      "[1350]\ttraining's binary_logloss: 0.300451\tvalid_1's binary_logloss: 0.301777\n",
      "[1400]\ttraining's binary_logloss: 0.300057\tvalid_1's binary_logloss: 0.30145\n",
      "[1450]\ttraining's binary_logloss: 0.299662\tvalid_1's binary_logloss: 0.301111\n",
      "[1500]\ttraining's binary_logloss: 0.299277\tvalid_1's binary_logloss: 0.300795\n",
      "[1550]\ttraining's binary_logloss: 0.298957\tvalid_1's binary_logloss: 0.300531\n",
      "[1600]\ttraining's binary_logloss: 0.298631\tvalid_1's binary_logloss: 0.300267\n",
      "[1650]\ttraining's binary_logloss: 0.298392\tvalid_1's binary_logloss: 0.3001\n",
      "[1700]\ttraining's binary_logloss: 0.298157\tvalid_1's binary_logloss: 0.299898\n",
      "[1750]\ttraining's binary_logloss: 0.297763\tvalid_1's binary_logloss: 0.29957\n",
      "[1800]\ttraining's binary_logloss: 0.297422\tvalid_1's binary_logloss: 0.299299\n",
      "[1850]\ttraining's binary_logloss: 0.297157\tvalid_1's binary_logloss: 0.299084\n",
      "[1900]\ttraining's binary_logloss: 0.29674\tvalid_1's binary_logloss: 0.298722\n",
      "[1950]\ttraining's binary_logloss: 0.296493\tvalid_1's binary_logloss: 0.29854\n",
      "[2000]\ttraining's binary_logloss: 0.296235\tvalid_1's binary_logloss: 0.298336\n",
      "[2050]\ttraining's binary_logloss: 0.296063\tvalid_1's binary_logloss: 0.298232\n",
      "[2100]\ttraining's binary_logloss: 0.29584\tvalid_1's binary_logloss: 0.298088\n",
      "[2150]\ttraining's binary_logloss: 0.295546\tvalid_1's binary_logloss: 0.297857\n",
      "[2200]\ttraining's binary_logloss: 0.295336\tvalid_1's binary_logloss: 0.29769\n",
      "[2250]\ttraining's binary_logloss: 0.295044\tvalid_1's binary_logloss: 0.297457\n",
      "[2300]\ttraining's binary_logloss: 0.29485\tvalid_1's binary_logloss: 0.297321\n",
      "[2350]\ttraining's binary_logloss: 0.294592\tvalid_1's binary_logloss: 0.297123\n",
      "[2400]\ttraining's binary_logloss: 0.294444\tvalid_1's binary_logloss: 0.297024\n",
      "[2450]\ttraining's binary_logloss: 0.294251\tvalid_1's binary_logloss: 0.296896\n",
      "[2500]\ttraining's binary_logloss: 0.293998\tvalid_1's binary_logloss: 0.296696\n",
      "[2550]\ttraining's binary_logloss: 0.293812\tvalid_1's binary_logloss: 0.296562\n",
      "[2600]\ttraining's binary_logloss: 0.293549\tvalid_1's binary_logloss: 0.296363\n",
      "[2650]\ttraining's binary_logloss: 0.293275\tvalid_1's binary_logloss: 0.29614\n",
      "[2700]\ttraining's binary_logloss: 0.293161\tvalid_1's binary_logloss: 0.296089\n",
      "[2750]\ttraining's binary_logloss: 0.293004\tvalid_1's binary_logloss: 0.296005\n",
      "[2800]\ttraining's binary_logloss: 0.292864\tvalid_1's binary_logloss: 0.295931\n",
      "[2850]\ttraining's binary_logloss: 0.292643\tvalid_1's binary_logloss: 0.295767\n",
      "[2900]\ttraining's binary_logloss: 0.292478\tvalid_1's binary_logloss: 0.295677\n",
      "[2950]\ttraining's binary_logloss: 0.29234\tvalid_1's binary_logloss: 0.295603\n",
      "[3000]\ttraining's binary_logloss: 0.292193\tvalid_1's binary_logloss: 0.295522\n",
      "[3050]\ttraining's binary_logloss: 0.292015\tvalid_1's binary_logloss: 0.295394\n",
      "[3100]\ttraining's binary_logloss: 0.291854\tvalid_1's binary_logloss: 0.295303\n",
      "[3150]\ttraining's binary_logloss: 0.291719\tvalid_1's binary_logloss: 0.295225\n",
      "[3200]\ttraining's binary_logloss: 0.291588\tvalid_1's binary_logloss: 0.29515\n",
      "[3250]\ttraining's binary_logloss: 0.291467\tvalid_1's binary_logloss: 0.295099\n",
      "[3300]\ttraining's binary_logloss: 0.291361\tvalid_1's binary_logloss: 0.295067\n",
      "[3350]\ttraining's binary_logloss: 0.29112\tvalid_1's binary_logloss: 0.294888\n",
      "[3400]\ttraining's binary_logloss: 0.290971\tvalid_1's binary_logloss: 0.29481\n",
      "[3450]\ttraining's binary_logloss: 0.290839\tvalid_1's binary_logloss: 0.294735\n",
      "[3500]\ttraining's binary_logloss: 0.290707\tvalid_1's binary_logloss: 0.294659\n",
      "[3550]\ttraining's binary_logloss: 0.290547\tvalid_1's binary_logloss: 0.294563\n",
      "[3600]\ttraining's binary_logloss: 0.290423\tvalid_1's binary_logloss: 0.294507\n",
      "[3650]\ttraining's binary_logloss: 0.290324\tvalid_1's binary_logloss: 0.294464\n",
      "[3700]\ttraining's binary_logloss: 0.29021\tvalid_1's binary_logloss: 0.294412\n",
      "[3750]\ttraining's binary_logloss: 0.290038\tvalid_1's binary_logloss: 0.294293\n",
      "[3800]\ttraining's binary_logloss: 0.289922\tvalid_1's binary_logloss: 0.294224\n",
      "[3850]\ttraining's binary_logloss: 0.28984\tvalid_1's binary_logloss: 0.294197\n",
      "[3900]\ttraining's binary_logloss: 0.289727\tvalid_1's binary_logloss: 0.294133\n",
      "[3950]\ttraining's binary_logloss: 0.289622\tvalid_1's binary_logloss: 0.294093\n",
      "[4000]\ttraining's binary_logloss: 0.289524\tvalid_1's binary_logloss: 0.294047\n",
      "[4050]\ttraining's binary_logloss: 0.2894\tvalid_1's binary_logloss: 0.293982\n",
      "[4100]\ttraining's binary_logloss: 0.28929\tvalid_1's binary_logloss: 0.293927\n",
      "[4150]\ttraining's binary_logloss: 0.289162\tvalid_1's binary_logloss: 0.293862\n",
      "[4200]\ttraining's binary_logloss: 0.289037\tvalid_1's binary_logloss: 0.293789\n",
      "[4250]\ttraining's binary_logloss: 0.288909\tvalid_1's binary_logloss: 0.293724\n",
      "[4300]\ttraining's binary_logloss: 0.288826\tvalid_1's binary_logloss: 0.293709\n",
      "[4350]\ttraining's binary_logloss: 0.288743\tvalid_1's binary_logloss: 0.293687\n",
      "[4400]\ttraining's binary_logloss: 0.288577\tvalid_1's binary_logloss: 0.293581\n",
      "[4450]\ttraining's binary_logloss: 0.288476\tvalid_1's binary_logloss: 0.293542\n",
      "[4500]\ttraining's binary_logloss: 0.288346\tvalid_1's binary_logloss: 0.293468\n",
      "[4550]\ttraining's binary_logloss: 0.288233\tvalid_1's binary_logloss: 0.293428\n",
      "[4600]\ttraining's binary_logloss: 0.288121\tvalid_1's binary_logloss: 0.29338\n",
      "[4650]\ttraining's binary_logloss: 0.287943\tvalid_1's binary_logloss: 0.293248\n",
      "[4700]\ttraining's binary_logloss: 0.28784\tvalid_1's binary_logloss: 0.293206\n",
      "[4750]\ttraining's binary_logloss: 0.287743\tvalid_1's binary_logloss: 0.293184\n",
      "[4800]\ttraining's binary_logloss: 0.287654\tvalid_1's binary_logloss: 0.293149\n",
      "[4850]\ttraining's binary_logloss: 0.287546\tvalid_1's binary_logloss: 0.293098\n",
      "[4900]\ttraining's binary_logloss: 0.287449\tvalid_1's binary_logloss: 0.293067\n",
      "[4950]\ttraining's binary_logloss: 0.287359\tvalid_1's binary_logloss: 0.293042\n",
      "[5000]\ttraining's binary_logloss: 0.287258\tvalid_1's binary_logloss: 0.293\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.287258\tvalid_1's binary_logloss: 0.293\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.357738\tvalid_1's binary_logloss: 0.357943\n",
      "[100]\ttraining's binary_logloss: 0.339916\tvalid_1's binary_logloss: 0.340175\n",
      "[150]\ttraining's binary_logloss: 0.331684\tvalid_1's binary_logloss: 0.331985\n",
      "[200]\ttraining's binary_logloss: 0.32606\tvalid_1's binary_logloss: 0.326465\n",
      "[250]\ttraining's binary_logloss: 0.322239\tvalid_1's binary_logloss: 0.322753\n",
      "[300]\ttraining's binary_logloss: 0.319496\tvalid_1's binary_logloss: 0.320101\n",
      "[350]\ttraining's binary_logloss: 0.317031\tvalid_1's binary_logloss: 0.317718\n",
      "[400]\ttraining's binary_logloss: 0.315216\tvalid_1's binary_logloss: 0.316012\n",
      "[450]\ttraining's binary_logloss: 0.313739\tvalid_1's binary_logloss: 0.314607\n",
      "[500]\ttraining's binary_logloss: 0.312051\tvalid_1's binary_logloss: 0.313\n",
      "[550]\ttraining's binary_logloss: 0.310792\tvalid_1's binary_logloss: 0.311831\n",
      "[600]\ttraining's binary_logloss: 0.309722\tvalid_1's binary_logloss: 0.310846\n",
      "[650]\ttraining's binary_logloss: 0.308719\tvalid_1's binary_logloss: 0.30994\n",
      "[700]\ttraining's binary_logloss: 0.307959\tvalid_1's binary_logloss: 0.309271\n",
      "[750]\ttraining's binary_logloss: 0.307002\tvalid_1's binary_logloss: 0.308409\n",
      "[800]\ttraining's binary_logloss: 0.306162\tvalid_1's binary_logloss: 0.307629\n",
      "[850]\ttraining's binary_logloss: 0.305558\tvalid_1's binary_logloss: 0.307104\n",
      "[900]\ttraining's binary_logloss: 0.304886\tvalid_1's binary_logloss: 0.306495\n",
      "[950]\ttraining's binary_logloss: 0.304238\tvalid_1's binary_logloss: 0.305922\n",
      "[1000]\ttraining's binary_logloss: 0.303797\tvalid_1's binary_logloss: 0.305552\n",
      "[1050]\ttraining's binary_logloss: 0.302965\tvalid_1's binary_logloss: 0.304772\n",
      "[1100]\ttraining's binary_logloss: 0.302408\tvalid_1's binary_logloss: 0.30429\n",
      "[1150]\ttraining's binary_logloss: 0.301862\tvalid_1's binary_logloss: 0.303817\n",
      "[1200]\ttraining's binary_logloss: 0.301271\tvalid_1's binary_logloss: 0.303305\n",
      "[1250]\ttraining's binary_logloss: 0.300885\tvalid_1's binary_logloss: 0.302981\n",
      "[1300]\ttraining's binary_logloss: 0.300424\tvalid_1's binary_logloss: 0.302595\n",
      "[1350]\ttraining's binary_logloss: 0.300139\tvalid_1's binary_logloss: 0.302377\n",
      "[1400]\ttraining's binary_logloss: 0.299735\tvalid_1's binary_logloss: 0.302017\n",
      "[1450]\ttraining's binary_logloss: 0.299347\tvalid_1's binary_logloss: 0.301699\n",
      "[1500]\ttraining's binary_logloss: 0.298932\tvalid_1's binary_logloss: 0.301334\n",
      "[1550]\ttraining's binary_logloss: 0.298618\tvalid_1's binary_logloss: 0.301063\n",
      "[1600]\ttraining's binary_logloss: 0.298368\tvalid_1's binary_logloss: 0.300874\n",
      "[1650]\ttraining's binary_logloss: 0.298034\tvalid_1's binary_logloss: 0.300597\n",
      "[1700]\ttraining's binary_logloss: 0.297649\tvalid_1's binary_logloss: 0.300281\n",
      "[1750]\ttraining's binary_logloss: 0.297242\tvalid_1's binary_logloss: 0.299928\n",
      "[1800]\ttraining's binary_logloss: 0.297017\tvalid_1's binary_logloss: 0.299775\n",
      "[1850]\ttraining's binary_logloss: 0.296732\tvalid_1's binary_logloss: 0.299549\n",
      "[1900]\ttraining's binary_logloss: 0.296457\tvalid_1's binary_logloss: 0.299347\n",
      "[1950]\ttraining's binary_logloss: 0.296259\tvalid_1's binary_logloss: 0.299207\n",
      "[2000]\ttraining's binary_logloss: 0.296049\tvalid_1's binary_logloss: 0.299073\n",
      "[2050]\ttraining's binary_logloss: 0.295833\tvalid_1's binary_logloss: 0.2989\n",
      "[2100]\ttraining's binary_logloss: 0.295616\tvalid_1's binary_logloss: 0.298743\n",
      "[2150]\ttraining's binary_logloss: 0.295337\tvalid_1's binary_logloss: 0.298529\n",
      "[2200]\ttraining's binary_logloss: 0.295024\tvalid_1's binary_logloss: 0.298275\n",
      "[2250]\ttraining's binary_logloss: 0.294816\tvalid_1's binary_logloss: 0.298128\n",
      "[2300]\ttraining's binary_logloss: 0.294611\tvalid_1's binary_logloss: 0.297999\n",
      "[2350]\ttraining's binary_logloss: 0.294445\tvalid_1's binary_logloss: 0.2979\n",
      "[2400]\ttraining's binary_logloss: 0.294195\tvalid_1's binary_logloss: 0.297713\n",
      "[2450]\ttraining's binary_logloss: 0.294006\tvalid_1's binary_logloss: 0.297582\n",
      "[2500]\ttraining's binary_logloss: 0.293882\tvalid_1's binary_logloss: 0.297513\n",
      "[2550]\ttraining's binary_logloss: 0.293618\tvalid_1's binary_logloss: 0.297306\n",
      "[2600]\ttraining's binary_logloss: 0.293478\tvalid_1's binary_logloss: 0.297243\n",
      "[2650]\ttraining's binary_logloss: 0.293283\tvalid_1's binary_logloss: 0.297104\n",
      "[2700]\ttraining's binary_logloss: 0.293083\tvalid_1's binary_logloss: 0.296974\n",
      "[2750]\ttraining's binary_logloss: 0.292905\tvalid_1's binary_logloss: 0.296865\n",
      "[2800]\ttraining's binary_logloss: 0.292694\tvalid_1's binary_logloss: 0.296713\n",
      "[2850]\ttraining's binary_logloss: 0.292412\tvalid_1's binary_logloss: 0.296482\n",
      "[2900]\ttraining's binary_logloss: 0.292253\tvalid_1's binary_logloss: 0.296386\n",
      "[2950]\ttraining's binary_logloss: 0.292074\tvalid_1's binary_logloss: 0.296273\n",
      "[3000]\ttraining's binary_logloss: 0.291907\tvalid_1's binary_logloss: 0.296185\n",
      "[3050]\ttraining's binary_logloss: 0.291745\tvalid_1's binary_logloss: 0.29608\n",
      "[3100]\ttraining's binary_logloss: 0.291601\tvalid_1's binary_logloss: 0.295995\n",
      "[3150]\ttraining's binary_logloss: 0.291477\tvalid_1's binary_logloss: 0.295931\n",
      "[3200]\ttraining's binary_logloss: 0.291311\tvalid_1's binary_logloss: 0.295829\n",
      "[3250]\ttraining's binary_logloss: 0.291194\tvalid_1's binary_logloss: 0.295787\n",
      "[3300]\ttraining's binary_logloss: 0.290988\tvalid_1's binary_logloss: 0.295634\n",
      "[3350]\ttraining's binary_logloss: 0.290859\tvalid_1's binary_logloss: 0.295567\n",
      "[3400]\ttraining's binary_logloss: 0.29066\tvalid_1's binary_logloss: 0.295423\n",
      "[3450]\ttraining's binary_logloss: 0.290528\tvalid_1's binary_logloss: 0.295358\n",
      "[3500]\ttraining's binary_logloss: 0.290398\tvalid_1's binary_logloss: 0.295291\n",
      "[3550]\ttraining's binary_logloss: 0.290287\tvalid_1's binary_logloss: 0.295231\n",
      "[3600]\ttraining's binary_logloss: 0.290135\tvalid_1's binary_logloss: 0.295144\n",
      "[3650]\ttraining's binary_logloss: 0.290029\tvalid_1's binary_logloss: 0.295102\n",
      "[3700]\ttraining's binary_logloss: 0.289893\tvalid_1's binary_logloss: 0.295025\n",
      "[3750]\ttraining's binary_logloss: 0.289784\tvalid_1's binary_logloss: 0.294986\n",
      "[3800]\ttraining's binary_logloss: 0.28965\tvalid_1's binary_logloss: 0.294904\n",
      "[3850]\ttraining's binary_logloss: 0.289501\tvalid_1's binary_logloss: 0.294824\n",
      "[3900]\ttraining's binary_logloss: 0.28938\tvalid_1's binary_logloss: 0.294764\n",
      "[3950]\ttraining's binary_logloss: 0.289263\tvalid_1's binary_logloss: 0.294713\n",
      "[4000]\ttraining's binary_logloss: 0.289153\tvalid_1's binary_logloss: 0.294662\n",
      "[4050]\ttraining's binary_logloss: 0.289017\tvalid_1's binary_logloss: 0.29459\n",
      "[4100]\ttraining's binary_logloss: 0.28892\tvalid_1's binary_logloss: 0.294563\n",
      "[4150]\ttraining's binary_logloss: 0.288814\tvalid_1's binary_logloss: 0.294518\n",
      "[4200]\ttraining's binary_logloss: 0.288713\tvalid_1's binary_logloss: 0.294477\n",
      "[4250]\ttraining's binary_logloss: 0.288617\tvalid_1's binary_logloss: 0.294451\n",
      "[4300]\ttraining's binary_logloss: 0.288544\tvalid_1's binary_logloss: 0.294435\n",
      "[4350]\ttraining's binary_logloss: 0.288432\tvalid_1's binary_logloss: 0.294381\n",
      "[4400]\ttraining's binary_logloss: 0.288344\tvalid_1's binary_logloss: 0.29435\n",
      "[4450]\ttraining's binary_logloss: 0.288226\tvalid_1's binary_logloss: 0.294295\n",
      "[4500]\ttraining's binary_logloss: 0.288108\tvalid_1's binary_logloss: 0.294233\n",
      "[4550]\ttraining's binary_logloss: 0.287973\tvalid_1's binary_logloss: 0.294153\n",
      "[4600]\ttraining's binary_logloss: 0.287849\tvalid_1's binary_logloss: 0.294088\n",
      "[4650]\ttraining's binary_logloss: 0.287725\tvalid_1's binary_logloss: 0.294008\n",
      "[4700]\ttraining's binary_logloss: 0.287641\tvalid_1's binary_logloss: 0.293994\n",
      "[4750]\ttraining's binary_logloss: 0.287544\tvalid_1's binary_logloss: 0.293968\n",
      "[4800]\ttraining's binary_logloss: 0.287426\tvalid_1's binary_logloss: 0.293899\n",
      "[4850]\ttraining's binary_logloss: 0.287276\tvalid_1's binary_logloss: 0.293811\n",
      "[4900]\ttraining's binary_logloss: 0.287188\tvalid_1's binary_logloss: 0.293786\n",
      "[4950]\ttraining's binary_logloss: 0.287057\tvalid_1's binary_logloss: 0.293706\n",
      "[5000]\ttraining's binary_logloss: 0.286966\tvalid_1's binary_logloss: 0.293684\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.286966\tvalid_1's binary_logloss: 0.293684\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.358192\tvalid_1's binary_logloss: 0.358416\n",
      "[100]\ttraining's binary_logloss: 0.34013\tvalid_1's binary_logloss: 0.340372\n",
      "[150]\ttraining's binary_logloss: 0.331601\tvalid_1's binary_logloss: 0.331866\n",
      "[200]\ttraining's binary_logloss: 0.326204\tvalid_1's binary_logloss: 0.326479\n",
      "[250]\ttraining's binary_logloss: 0.322483\tvalid_1's binary_logloss: 0.322775\n",
      "[300]\ttraining's binary_logloss: 0.319527\tvalid_1's binary_logloss: 0.319853\n",
      "[350]\ttraining's binary_logloss: 0.317155\tvalid_1's binary_logloss: 0.317534\n",
      "[400]\ttraining's binary_logloss: 0.315267\tvalid_1's binary_logloss: 0.315695\n",
      "[450]\ttraining's binary_logloss: 0.313699\tvalid_1's binary_logloss: 0.314179\n",
      "[500]\ttraining's binary_logloss: 0.312271\tvalid_1's binary_logloss: 0.312815\n",
      "[550]\ttraining's binary_logloss: 0.310963\tvalid_1's binary_logloss: 0.311588\n",
      "[600]\ttraining's binary_logloss: 0.309927\tvalid_1's binary_logloss: 0.310621\n",
      "[650]\ttraining's binary_logloss: 0.308753\tvalid_1's binary_logloss: 0.309513\n",
      "[700]\ttraining's binary_logloss: 0.307924\tvalid_1's binary_logloss: 0.308741\n",
      "[750]\ttraining's binary_logloss: 0.307041\tvalid_1's binary_logloss: 0.307907\n",
      "[800]\ttraining's binary_logloss: 0.306378\tvalid_1's binary_logloss: 0.307306\n",
      "[850]\ttraining's binary_logloss: 0.305706\tvalid_1's binary_logloss: 0.306692\n",
      "[900]\ttraining's binary_logloss: 0.304922\tvalid_1's binary_logloss: 0.305968\n",
      "[950]\ttraining's binary_logloss: 0.304231\tvalid_1's binary_logloss: 0.305355\n",
      "[1000]\ttraining's binary_logloss: 0.303719\tvalid_1's binary_logloss: 0.304918\n",
      "[1050]\ttraining's binary_logloss: 0.3033\tvalid_1's binary_logloss: 0.304563\n",
      "[1100]\ttraining's binary_logloss: 0.302775\tvalid_1's binary_logloss: 0.304102\n",
      "[1150]\ttraining's binary_logloss: 0.302167\tvalid_1's binary_logloss: 0.303555\n",
      "[1200]\ttraining's binary_logloss: 0.301651\tvalid_1's binary_logloss: 0.303076\n",
      "[1250]\ttraining's binary_logloss: 0.301173\tvalid_1's binary_logloss: 0.302665\n",
      "[1300]\ttraining's binary_logloss: 0.300817\tvalid_1's binary_logloss: 0.302377\n",
      "[1350]\ttraining's binary_logloss: 0.300454\tvalid_1's binary_logloss: 0.302074\n",
      "[1400]\ttraining's binary_logloss: 0.300068\tvalid_1's binary_logloss: 0.301738\n",
      "[1450]\ttraining's binary_logloss: 0.299537\tvalid_1's binary_logloss: 0.30126\n",
      "[1500]\ttraining's binary_logloss: 0.299137\tvalid_1's binary_logloss: 0.300916\n",
      "[1550]\ttraining's binary_logloss: 0.298779\tvalid_1's binary_logloss: 0.300627\n",
      "[1600]\ttraining's binary_logloss: 0.298468\tvalid_1's binary_logloss: 0.300375\n",
      "[1650]\ttraining's binary_logloss: 0.298205\tvalid_1's binary_logloss: 0.300176\n",
      "[1700]\ttraining's binary_logloss: 0.297947\tvalid_1's binary_logloss: 0.299975\n",
      "[1750]\ttraining's binary_logloss: 0.29769\tvalid_1's binary_logloss: 0.299771\n",
      "[1800]\ttraining's binary_logloss: 0.297383\tvalid_1's binary_logloss: 0.29953\n",
      "[1850]\ttraining's binary_logloss: 0.297097\tvalid_1's binary_logloss: 0.299315\n",
      "[1900]\ttraining's binary_logloss: 0.296756\tvalid_1's binary_logloss: 0.299018\n",
      "[1950]\ttraining's binary_logloss: 0.296536\tvalid_1's binary_logloss: 0.298854\n",
      "[2000]\ttraining's binary_logloss: 0.296183\tvalid_1's binary_logloss: 0.298558\n",
      "[2050]\ttraining's binary_logloss: 0.295819\tvalid_1's binary_logloss: 0.298242\n",
      "[2100]\ttraining's binary_logloss: 0.295658\tvalid_1's binary_logloss: 0.298141\n",
      "[2150]\ttraining's binary_logloss: 0.295428\tvalid_1's binary_logloss: 0.29798\n",
      "[2200]\ttraining's binary_logloss: 0.295142\tvalid_1's binary_logloss: 0.297753\n",
      "[2250]\ttraining's binary_logloss: 0.294941\tvalid_1's binary_logloss: 0.297616\n",
      "[2300]\ttraining's binary_logloss: 0.294699\tvalid_1's binary_logloss: 0.297431\n",
      "[2350]\ttraining's binary_logloss: 0.294531\tvalid_1's binary_logloss: 0.297322\n",
      "[2400]\ttraining's binary_logloss: 0.294262\tvalid_1's binary_logloss: 0.29712\n",
      "[2450]\ttraining's binary_logloss: 0.294038\tvalid_1's binary_logloss: 0.296964\n",
      "[2500]\ttraining's binary_logloss: 0.293866\tvalid_1's binary_logloss: 0.29687\n",
      "[2550]\ttraining's binary_logloss: 0.29368\tvalid_1's binary_logloss: 0.296752\n",
      "[2600]\ttraining's binary_logloss: 0.293497\tvalid_1's binary_logloss: 0.296631\n",
      "[2650]\ttraining's binary_logloss: 0.293242\tvalid_1's binary_logloss: 0.296425\n",
      "[2700]\ttraining's binary_logloss: 0.293044\tvalid_1's binary_logloss: 0.29628\n",
      "[2750]\ttraining's binary_logloss: 0.292793\tvalid_1's binary_logloss: 0.296102\n",
      "[2800]\ttraining's binary_logloss: 0.292619\tvalid_1's binary_logloss: 0.295989\n",
      "[2850]\ttraining's binary_logloss: 0.292468\tvalid_1's binary_logloss: 0.295908\n",
      "[2900]\ttraining's binary_logloss: 0.292328\tvalid_1's binary_logloss: 0.295836\n",
      "[2950]\ttraining's binary_logloss: 0.292182\tvalid_1's binary_logloss: 0.295758\n",
      "[3000]\ttraining's binary_logloss: 0.292009\tvalid_1's binary_logloss: 0.295651\n",
      "[3050]\ttraining's binary_logloss: 0.291817\tvalid_1's binary_logloss: 0.295521\n",
      "[3100]\ttraining's binary_logloss: 0.29169\tvalid_1's binary_logloss: 0.295459\n",
      "[3150]\ttraining's binary_logloss: 0.291579\tvalid_1's binary_logloss: 0.295409\n",
      "[3200]\ttraining's binary_logloss: 0.29145\tvalid_1's binary_logloss: 0.295335\n",
      "[3250]\ttraining's binary_logloss: 0.291283\tvalid_1's binary_logloss: 0.295235\n",
      "[3300]\ttraining's binary_logloss: 0.29118\tvalid_1's binary_logloss: 0.295185\n",
      "[3350]\ttraining's binary_logloss: 0.291078\tvalid_1's binary_logloss: 0.295143\n",
      "[3400]\ttraining's binary_logloss: 0.290926\tvalid_1's binary_logloss: 0.295048\n",
      "[3450]\ttraining's binary_logloss: 0.290801\tvalid_1's binary_logloss: 0.294985\n",
      "[3500]\ttraining's binary_logloss: 0.290685\tvalid_1's binary_logloss: 0.294943\n",
      "[3550]\ttraining's binary_logloss: 0.290576\tvalid_1's binary_logloss: 0.2949\n",
      "[3600]\ttraining's binary_logloss: 0.290382\tvalid_1's binary_logloss: 0.294769\n",
      "[3650]\ttraining's binary_logloss: 0.290245\tvalid_1's binary_logloss: 0.294685\n",
      "[3700]\ttraining's binary_logloss: 0.290089\tvalid_1's binary_logloss: 0.294583\n",
      "[3750]\ttraining's binary_logloss: 0.289965\tvalid_1's binary_logloss: 0.294524\n",
      "[3800]\ttraining's binary_logloss: 0.289836\tvalid_1's binary_logloss: 0.294465\n",
      "[3850]\ttraining's binary_logloss: 0.289702\tvalid_1's binary_logloss: 0.294387\n",
      "[3900]\ttraining's binary_logloss: 0.289608\tvalid_1's binary_logloss: 0.294356\n",
      "[3950]\ttraining's binary_logloss: 0.289513\tvalid_1's binary_logloss: 0.294326\n",
      "[4000]\ttraining's binary_logloss: 0.289414\tvalid_1's binary_logloss: 0.294301\n",
      "[4050]\ttraining's binary_logloss: 0.289325\tvalid_1's binary_logloss: 0.294279\n",
      "[4100]\ttraining's binary_logloss: 0.289057\tvalid_1's binary_logloss: 0.294073\n",
      "[4150]\ttraining's binary_logloss: 0.288928\tvalid_1's binary_logloss: 0.294006\n",
      "[4200]\ttraining's binary_logloss: 0.288839\tvalid_1's binary_logloss: 0.293974\n",
      "[4250]\ttraining's binary_logloss: 0.288721\tvalid_1's binary_logloss: 0.293919\n",
      "[4300]\ttraining's binary_logloss: 0.288627\tvalid_1's binary_logloss: 0.293893\n",
      "[4350]\ttraining's binary_logloss: 0.288526\tvalid_1's binary_logloss: 0.293865\n",
      "[4400]\ttraining's binary_logloss: 0.288431\tvalid_1's binary_logloss: 0.293819\n",
      "[4450]\ttraining's binary_logloss: 0.28829\tvalid_1's binary_logloss: 0.293732\n",
      "[4500]\ttraining's binary_logloss: 0.288198\tvalid_1's binary_logloss: 0.293696\n",
      "[4550]\ttraining's binary_logloss: 0.288083\tvalid_1's binary_logloss: 0.293646\n",
      "[4600]\ttraining's binary_logloss: 0.287992\tvalid_1's binary_logloss: 0.293624\n",
      "[4650]\ttraining's binary_logloss: 0.287895\tvalid_1's binary_logloss: 0.293579\n",
      "[4700]\ttraining's binary_logloss: 0.287793\tvalid_1's binary_logloss: 0.293543\n",
      "[4750]\ttraining's binary_logloss: 0.287711\tvalid_1's binary_logloss: 0.293522\n",
      "[4800]\ttraining's binary_logloss: 0.287625\tvalid_1's binary_logloss: 0.293499\n",
      "[4850]\ttraining's binary_logloss: 0.287538\tvalid_1's binary_logloss: 0.293481\n",
      "[4900]\ttraining's binary_logloss: 0.28742\tvalid_1's binary_logloss: 0.293429\n",
      "[4950]\ttraining's binary_logloss: 0.287335\tvalid_1's binary_logloss: 0.293393\n",
      "[5000]\ttraining's binary_logloss: 0.287182\tvalid_1's binary_logloss: 0.293295\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.287182\tvalid_1's binary_logloss: 0.293295\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.358511\tvalid_1's binary_logloss: 0.358787\n",
      "[100]\ttraining's binary_logloss: 0.340162\tvalid_1's binary_logloss: 0.340603\n",
      "[150]\ttraining's binary_logloss: 0.33173\tvalid_1's binary_logloss: 0.332285\n",
      "[200]\ttraining's binary_logloss: 0.32623\tvalid_1's binary_logloss: 0.326873\n",
      "[250]\ttraining's binary_logloss: 0.322072\tvalid_1's binary_logloss: 0.322813\n",
      "[300]\ttraining's binary_logloss: 0.319437\tvalid_1's binary_logloss: 0.320259\n",
      "[350]\ttraining's binary_logloss: 0.316872\tvalid_1's binary_logloss: 0.317784\n",
      "[400]\ttraining's binary_logloss: 0.315048\tvalid_1's binary_logloss: 0.316045\n",
      "[450]\ttraining's binary_logloss: 0.313552\tvalid_1's binary_logloss: 0.314627\n",
      "[500]\ttraining's binary_logloss: 0.311974\tvalid_1's binary_logloss: 0.313117\n",
      "[550]\ttraining's binary_logloss: 0.310662\tvalid_1's binary_logloss: 0.311883\n",
      "[600]\ttraining's binary_logloss: 0.309217\tvalid_1's binary_logloss: 0.31052\n",
      "[650]\ttraining's binary_logloss: 0.308313\tvalid_1's binary_logloss: 0.309702\n",
      "[700]\ttraining's binary_logloss: 0.307671\tvalid_1's binary_logloss: 0.309138\n",
      "[750]\ttraining's binary_logloss: 0.306765\tvalid_1's binary_logloss: 0.308314\n",
      "[800]\ttraining's binary_logloss: 0.305831\tvalid_1's binary_logloss: 0.307466\n",
      "[850]\ttraining's binary_logloss: 0.305297\tvalid_1's binary_logloss: 0.307011\n",
      "[900]\ttraining's binary_logloss: 0.304795\tvalid_1's binary_logloss: 0.306595\n",
      "[950]\ttraining's binary_logloss: 0.304049\tvalid_1's binary_logloss: 0.305902\n",
      "[1000]\ttraining's binary_logloss: 0.303525\tvalid_1's binary_logloss: 0.305436\n",
      "[1050]\ttraining's binary_logloss: 0.303015\tvalid_1's binary_logloss: 0.304988\n",
      "[1100]\ttraining's binary_logloss: 0.30239\tvalid_1's binary_logloss: 0.304423\n",
      "[1150]\ttraining's binary_logloss: 0.301935\tvalid_1's binary_logloss: 0.304047\n",
      "[1200]\ttraining's binary_logloss: 0.301462\tvalid_1's binary_logloss: 0.303624\n",
      "[1250]\ttraining's binary_logloss: 0.301139\tvalid_1's binary_logloss: 0.303371\n",
      "[1300]\ttraining's binary_logloss: 0.300592\tvalid_1's binary_logloss: 0.302874\n",
      "[1350]\ttraining's binary_logloss: 0.300198\tvalid_1's binary_logloss: 0.302551\n",
      "[1400]\ttraining's binary_logloss: 0.299567\tvalid_1's binary_logloss: 0.301978\n",
      "[1450]\ttraining's binary_logloss: 0.29922\tvalid_1's binary_logloss: 0.301688\n",
      "[1500]\ttraining's binary_logloss: 0.298916\tvalid_1's binary_logloss: 0.301452\n",
      "[1550]\ttraining's binary_logloss: 0.298571\tvalid_1's binary_logloss: 0.301186\n",
      "[1600]\ttraining's binary_logloss: 0.298251\tvalid_1's binary_logloss: 0.30094\n",
      "[1650]\ttraining's binary_logloss: 0.297947\tvalid_1's binary_logloss: 0.30069\n",
      "[1700]\ttraining's binary_logloss: 0.297637\tvalid_1's binary_logloss: 0.300444\n",
      "[1750]\ttraining's binary_logloss: 0.297332\tvalid_1's binary_logloss: 0.300221\n",
      "[1800]\ttraining's binary_logloss: 0.297005\tvalid_1's binary_logloss: 0.299962\n",
      "[1850]\ttraining's binary_logloss: 0.296736\tvalid_1's binary_logloss: 0.299753\n",
      "[1900]\ttraining's binary_logloss: 0.296422\tvalid_1's binary_logloss: 0.299508\n",
      "[1950]\ttraining's binary_logloss: 0.296217\tvalid_1's binary_logloss: 0.299361\n",
      "[2000]\ttraining's binary_logloss: 0.295963\tvalid_1's binary_logloss: 0.299168\n",
      "[2050]\ttraining's binary_logloss: 0.295681\tvalid_1's binary_logloss: 0.298948\n",
      "[2100]\ttraining's binary_logloss: 0.295491\tvalid_1's binary_logloss: 0.298797\n",
      "[2150]\ttraining's binary_logloss: 0.295222\tvalid_1's binary_logloss: 0.2986\n",
      "[2200]\ttraining's binary_logloss: 0.295018\tvalid_1's binary_logloss: 0.29847\n",
      "[2250]\ttraining's binary_logloss: 0.29483\tvalid_1's binary_logloss: 0.298349\n",
      "[2300]\ttraining's binary_logloss: 0.294588\tvalid_1's binary_logloss: 0.298169\n",
      "[2350]\ttraining's binary_logloss: 0.294212\tvalid_1's binary_logloss: 0.29784\n",
      "[2400]\ttraining's binary_logloss: 0.293966\tvalid_1's binary_logloss: 0.297663\n",
      "[2450]\ttraining's binary_logloss: 0.293774\tvalid_1's binary_logloss: 0.297547\n",
      "[2500]\ttraining's binary_logloss: 0.293593\tvalid_1's binary_logloss: 0.297444\n",
      "[2550]\ttraining's binary_logloss: 0.293417\tvalid_1's binary_logloss: 0.297329\n",
      "[2600]\ttraining's binary_logloss: 0.293229\tvalid_1's binary_logloss: 0.297216\n",
      "[2650]\ttraining's binary_logloss: 0.293055\tvalid_1's binary_logloss: 0.297106\n",
      "[2700]\ttraining's binary_logloss: 0.292875\tvalid_1's binary_logloss: 0.296985\n",
      "[2750]\ttraining's binary_logloss: 0.292751\tvalid_1's binary_logloss: 0.296931\n",
      "[2800]\ttraining's binary_logloss: 0.292608\tvalid_1's binary_logloss: 0.296865\n",
      "[2850]\ttraining's binary_logloss: 0.292482\tvalid_1's binary_logloss: 0.296806\n",
      "[2900]\ttraining's binary_logloss: 0.292193\tvalid_1's binary_logloss: 0.296584\n",
      "[2950]\ttraining's binary_logloss: 0.292062\tvalid_1's binary_logloss: 0.29651\n",
      "[3000]\ttraining's binary_logloss: 0.291936\tvalid_1's binary_logloss: 0.296448\n",
      "[3050]\ttraining's binary_logloss: 0.291767\tvalid_1's binary_logloss: 0.296328\n",
      "[3100]\ttraining's binary_logloss: 0.291607\tvalid_1's binary_logloss: 0.296228\n",
      "[3150]\ttraining's binary_logloss: 0.291512\tvalid_1's binary_logloss: 0.296193\n",
      "[3200]\ttraining's binary_logloss: 0.291336\tvalid_1's binary_logloss: 0.296084\n",
      "[3250]\ttraining's binary_logloss: 0.291211\tvalid_1's binary_logloss: 0.296012\n",
      "[3300]\ttraining's binary_logloss: 0.291052\tvalid_1's binary_logloss: 0.295908\n",
      "[3350]\ttraining's binary_logloss: 0.290904\tvalid_1's binary_logloss: 0.295827\n",
      "[3400]\ttraining's binary_logloss: 0.290801\tvalid_1's binary_logloss: 0.295791\n",
      "[3450]\ttraining's binary_logloss: 0.290684\tvalid_1's binary_logloss: 0.295737\n",
      "[3500]\ttraining's binary_logloss: 0.29055\tvalid_1's binary_logloss: 0.295666\n",
      "[3550]\ttraining's binary_logloss: 0.290395\tvalid_1's binary_logloss: 0.295576\n",
      "[3600]\ttraining's binary_logloss: 0.290244\tvalid_1's binary_logloss: 0.295471\n",
      "[3650]\ttraining's binary_logloss: 0.290118\tvalid_1's binary_logloss: 0.295409\n",
      "[3700]\ttraining's binary_logloss: 0.290004\tvalid_1's binary_logloss: 0.295368\n",
      "[3750]\ttraining's binary_logloss: 0.289893\tvalid_1's binary_logloss: 0.295318\n",
      "[3800]\ttraining's binary_logloss: 0.289784\tvalid_1's binary_logloss: 0.295269\n",
      "[3850]\ttraining's binary_logloss: 0.289692\tvalid_1's binary_logloss: 0.295246\n",
      "[3900]\ttraining's binary_logloss: 0.2896\tvalid_1's binary_logloss: 0.295206\n",
      "[3950]\ttraining's binary_logloss: 0.289447\tvalid_1's binary_logloss: 0.295096\n",
      "[4000]\ttraining's binary_logloss: 0.289331\tvalid_1's binary_logloss: 0.295048\n",
      "[4050]\ttraining's binary_logloss: 0.289239\tvalid_1's binary_logloss: 0.295019\n",
      "[4100]\ttraining's binary_logloss: 0.289128\tvalid_1's binary_logloss: 0.294973\n",
      "[4150]\ttraining's binary_logloss: 0.289013\tvalid_1's binary_logloss: 0.294924\n",
      "[4200]\ttraining's binary_logloss: 0.288889\tvalid_1's binary_logloss: 0.294859\n",
      "[4250]\ttraining's binary_logloss: 0.28876\tvalid_1's binary_logloss: 0.294796\n",
      "[4300]\ttraining's binary_logloss: 0.288626\tvalid_1's binary_logloss: 0.294729\n",
      "[4350]\ttraining's binary_logloss: 0.288528\tvalid_1's binary_logloss: 0.294694\n",
      "[4400]\ttraining's binary_logloss: 0.28844\tvalid_1's binary_logloss: 0.294661\n",
      "[4450]\ttraining's binary_logloss: 0.288313\tvalid_1's binary_logloss: 0.294584\n",
      "[4500]\ttraining's binary_logloss: 0.288168\tvalid_1's binary_logloss: 0.294493\n",
      "[4550]\ttraining's binary_logloss: 0.288073\tvalid_1's binary_logloss: 0.294458\n",
      "[4600]\ttraining's binary_logloss: 0.287981\tvalid_1's binary_logloss: 0.294433\n",
      "[4650]\ttraining's binary_logloss: 0.287881\tvalid_1's binary_logloss: 0.294396\n",
      "[4700]\ttraining's binary_logloss: 0.287762\tvalid_1's binary_logloss: 0.294336\n",
      "[4750]\ttraining's binary_logloss: 0.287664\tvalid_1's binary_logloss: 0.294293\n",
      "[4800]\ttraining's binary_logloss: 0.287545\tvalid_1's binary_logloss: 0.294229\n",
      "[4850]\ttraining's binary_logloss: 0.287441\tvalid_1's binary_logloss: 0.294184\n",
      "[4900]\ttraining's binary_logloss: 0.287314\tvalid_1's binary_logloss: 0.294126\n",
      "[4950]\ttraining's binary_logloss: 0.287194\tvalid_1's binary_logloss: 0.294068\n",
      "[5000]\ttraining's binary_logloss: 0.287111\tvalid_1's binary_logloss: 0.294041\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's binary_logloss: 0.287111\tvalid_1's binary_logloss: 0.294041\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 500 rounds\n",
      "[50]\ttraining's binary_logloss: 0.330219\tvalid_1's binary_logloss: 0.332283\n",
      "[100]\ttraining's binary_logloss: 0.304225\tvalid_1's binary_logloss: 0.308273\n",
      "[150]\ttraining's binary_logloss: 0.294254\tvalid_1's binary_logloss: 0.30042\n",
      "[200]\ttraining's binary_logloss: 0.288362\tvalid_1's binary_logloss: 0.296818\n",
      "[250]\ttraining's binary_logloss: 0.283476\tvalid_1's binary_logloss: 0.294143\n",
      "[300]\ttraining's binary_logloss: 0.280302\tvalid_1's binary_logloss: 0.29327\n",
      "[350]\ttraining's binary_logloss: 0.276655\tvalid_1's binary_logloss: 0.291802\n",
      "[400]\ttraining's binary_logloss: 0.273832\tvalid_1's binary_logloss: 0.291144\n",
      "[450]\ttraining's binary_logloss: 0.271248\tvalid_1's binary_logloss: 0.290614\n",
      "[500]\ttraining's binary_logloss: 0.268815\tvalid_1's binary_logloss: 0.29018\n",
      "[550]\ttraining's binary_logloss: 0.266493\tvalid_1's binary_logloss: 0.289888\n",
      "[600]\ttraining's binary_logloss: 0.264273\tvalid_1's binary_logloss: 0.289626\n",
      "[650]\ttraining's binary_logloss: 0.262401\tvalid_1's binary_logloss: 0.289436\n",
      "[700]\ttraining's binary_logloss: 0.260189\tvalid_1's binary_logloss: 0.289118\n",
      "[750]\ttraining's binary_logloss: 0.258351\tvalid_1's binary_logloss: 0.288991\n",
      "[800]\ttraining's binary_logloss: 0.25643\tvalid_1's binary_logloss: 0.288846\n",
      "[850]\ttraining's binary_logloss: 0.254564\tvalid_1's binary_logloss: 0.288663\n",
      "[900]\ttraining's binary_logloss: 0.252843\tvalid_1's binary_logloss: 0.288588\n",
      "[950]\ttraining's binary_logloss: 0.251139\tvalid_1's binary_logloss: 0.28855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-05 02:06:47,142]\u001b[0m Trial 3 failed with parameters: {'n_estimators': 10000, 'learning_rate': 0.025, 'num_leaves': 904, 'max_depth': 15, 'min_split_gain': 0.30000000000000004, 'subsample': 0.8, 'subsample_freq': 10, 'colsample_bytree': 0.6, 'alpha': 0.4, 'lambda': 4.0} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-26-ad67c2573925>\", line 4, in <lambda>\n",
      "    func = lambda trial: objective(trial, train, label)\n",
      "  File \"<ipython-input-24-8a49b3a4229c>\", line 38, in objective\n",
      "    clf = lgb.train(\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/engine.py\", line 292, in train\n",
      "    booster.update(fobj=fobj)\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\", line 3021, in update\n",
      "    _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-05 02:06:47,155]\u001b[0m Trial 3 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ad67c2573925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\tBest value (LogLoss): {study_lgb.best_value:.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ad67c2573925>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\", sampler=optuna.samplers.GridSampler())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstudy_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8a49b3a4229c>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, train, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# num_round = 10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         clf = lgb.train(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    290\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3020\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot update due to null objective function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3021\u001b[0;31m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[1;32m   3022\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study_lgb = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Reg is_installed\", sampler=optuna.samplers.TPESampler())\n",
    "\n",
    "func = lambda trial: objective(trial, train, label)\n",
    "study_lgb.optimize(func, n_trials=500)\n",
    "\n",
    "print(f\"\\tBest value (LogLoss): {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 4.0461, AUC: 0.8730, Precision: 0.8973, Recall: 0.5275, F1 Score: 0.6644\n"
     ]
    }
   ],
   "source": [
    "logloss = metrics.log_loss(label, [1 if i >= 0.5 else 0 for i in oof_lgb_cli], labels=[0, 1])\n",
    "acc = metrics.roc_auc_score(label, oof_lgb_cli)\n",
    "precision = metrics.precision_score(label, [1 if i >= 0.5 else 0 for i in oof_lgb_cli])\n",
    "recall = metrics.recall_score(label, [1 if i >= 0.5 else 0 for i in oof_lgb_cli])\n",
    "f1 = metrics.f1_score(label, [1 if i >= 0.5 else 0 for i in oof_lgb_cli])\n",
    "\n",
    "print(f\"Logloss: {logloss:.4f}, AUC: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68440"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_clicked_lgb = [1 if i >= best_threshold_cli else 0 for i in predictions_lgb_cli]\n",
    "sum(is_clicked_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68440"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(is_clicked_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       79.000000\n",
       "mean      2368.427848\n",
       "std       2512.904295\n",
       "min          0.000000\n",
       "25%        306.400000\n",
       "50%       1528.600000\n",
       "75%       3867.800000\n",
       "max      10167.800000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_imp_cli = pd.DataFrame(feat_imp_lgb_cli, columns = features).apply(np.mean, axis = 0).sort_values(ascending=False)\n",
    "avg_imp_cli.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_4     10167.8\n",
       "f_11     9483.6\n",
       "f_15     9031.4\n",
       "f_42     8753.8\n",
       "f_6      8142.2\n",
       "         ...   \n",
       "f_29        0.0\n",
       "f_7         0.0\n",
       "f_28        0.0\n",
       "f_27        0.0\n",
       "f_26        0.0\n",
       "Length: 79, dtype: float64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_imp_cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is_installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = is_installed_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "fold n°0\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's auc: 0.875921\tvalid_1's auc: 0.873932\n",
      "[600]\ttraining's auc: 0.883247\tvalid_1's auc: 0.878936\n",
      "[900]\ttraining's auc: 0.887544\tvalid_1's auc: 0.880815\n",
      "[1200]\ttraining's auc: 0.891024\tvalid_1's auc: 0.881982\n",
      "[1500]\ttraining's auc: 0.893886\tvalid_1's auc: 0.882578\n",
      "[1800]\ttraining's auc: 0.896527\tvalid_1's auc: 0.883067\n",
      "[2100]\ttraining's auc: 0.898997\tvalid_1's auc: 0.883451\n",
      "[2400]\ttraining's auc: 0.901315\tvalid_1's auc: 0.883738\n",
      "[2700]\ttraining's auc: 0.9035\tvalid_1's auc: 0.883952\n",
      "[3000]\ttraining's auc: 0.905566\tvalid_1's auc: 0.884137\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.905566\tvalid_1's auc: 0.884137\n",
      "fold n°1\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's auc: 0.876045\tvalid_1's auc: 0.873745\n",
      "[600]\ttraining's auc: 0.883256\tvalid_1's auc: 0.878702\n",
      "[900]\ttraining's auc: 0.887612\tvalid_1's auc: 0.880688\n",
      "[1200]\ttraining's auc: 0.891033\tvalid_1's auc: 0.881759\n",
      "[1500]\ttraining's auc: 0.893979\tvalid_1's auc: 0.882525\n",
      "[1800]\ttraining's auc: 0.896637\tvalid_1's auc: 0.883047\n",
      "[2100]\ttraining's auc: 0.899079\tvalid_1's auc: 0.883322\n",
      "[2400]\ttraining's auc: 0.90135\tvalid_1's auc: 0.883622\n",
      "[2700]\ttraining's auc: 0.903492\tvalid_1's auc: 0.883825\n",
      "[3000]\ttraining's auc: 0.905559\tvalid_1's auc: 0.884003\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.905559\tvalid_1's auc: 0.884003\n",
      "fold n°2\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's auc: 0.875792\tvalid_1's auc: 0.873234\n",
      "[600]\ttraining's auc: 0.88312\tvalid_1's auc: 0.878354\n",
      "[900]\ttraining's auc: 0.88747\tvalid_1's auc: 0.880282\n",
      "[1200]\ttraining's auc: 0.890995\tvalid_1's auc: 0.881471\n",
      "[1500]\ttraining's auc: 0.894053\tvalid_1's auc: 0.882351\n",
      "[1800]\ttraining's auc: 0.896666\tvalid_1's auc: 0.882843\n",
      "[2100]\ttraining's auc: 0.899139\tvalid_1's auc: 0.883144\n",
      "[2400]\ttraining's auc: 0.90139\tvalid_1's auc: 0.883427\n",
      "[2700]\ttraining's auc: 0.903578\tvalid_1's auc: 0.883669\n",
      "[3000]\ttraining's auc: 0.905657\tvalid_1's auc: 0.88385\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.905657\tvalid_1's auc: 0.88385\n",
      "fold n°3\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's auc: 0.875528\tvalid_1's auc: 0.873333\n",
      "[600]\ttraining's auc: 0.883251\tvalid_1's auc: 0.878651\n",
      "[900]\ttraining's auc: 0.887656\tvalid_1's auc: 0.880602\n",
      "[1200]\ttraining's auc: 0.891029\tvalid_1's auc: 0.881607\n",
      "[1500]\ttraining's auc: 0.894034\tvalid_1's auc: 0.882344\n",
      "[1800]\ttraining's auc: 0.896671\tvalid_1's auc: 0.882797\n",
      "[2100]\ttraining's auc: 0.899128\tvalid_1's auc: 0.883196\n",
      "[2400]\ttraining's auc: 0.901457\tvalid_1's auc: 0.883462\n",
      "[2700]\ttraining's auc: 0.90365\tvalid_1's auc: 0.883684\n",
      "[3000]\ttraining's auc: 0.905751\tvalid_1's auc: 0.883816\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.905751\tvalid_1's auc: 0.883816\n",
      "fold n°4\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's auc: 0.875965\tvalid_1's auc: 0.873229\n",
      "[600]\ttraining's auc: 0.883503\tvalid_1's auc: 0.878401\n",
      "[900]\ttraining's auc: 0.887781\tvalid_1's auc: 0.880201\n",
      "[1200]\ttraining's auc: 0.891095\tvalid_1's auc: 0.881178\n",
      "[1500]\ttraining's auc: 0.894014\tvalid_1's auc: 0.881899\n",
      "[1800]\ttraining's auc: 0.896614\tvalid_1's auc: 0.882352\n",
      "[2100]\ttraining's auc: 0.899156\tvalid_1's auc: 0.882806\n",
      "[2400]\ttraining's auc: 0.901464\tvalid_1's auc: 0.883102\n",
      "[2700]\ttraining's auc: 0.903653\tvalid_1's auc: 0.883272\n",
      "[3000]\ttraining's auc: 0.905741\tvalid_1's auc: 0.883482\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[3000]\ttraining's auc: 0.905741\tvalid_1's auc: 0.883482\n"
     ]
    }
   ],
   "source": [
    "label = is_installed_label\n",
    "\n",
    "KF = StratifiedKFold(n_splits=K, random_state=seed, shuffle=True)\n",
    "feat_imp_lgb_ins = list()\n",
    "\n",
    "oof_lgb_ins = np.zeros(len(train))\n",
    "predictions_lgb_ins = np.zeros((len(test)))\n",
    "print(len(features))\n",
    "\n",
    "# 模型训练\n",
    "for fold_, (trn_idx, val_idx) in enumerate(KF.split(train.values, label.values)):\n",
    "    print(\"fold n°{}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=label.iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(train.iloc[val_idx][features], label=label.iloc[val_idx])\n",
    "    \n",
    "    num_round = 3000\n",
    "    clf = lgb.train(\n",
    "        lgb_params,\n",
    "        trn_data,\n",
    "        num_round,\n",
    "        valid_sets=[trn_data, val_data],\n",
    "        verbose_eval=300,\n",
    "        early_stopping_rounds=100,\n",
    "    )\n",
    "\n",
    "    oof_lgb_ins[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n",
    "    predictions_lgb_ins[:] += clf.predict(test[features], num_iteration=clf.best_iteration) / K\n",
    "    feat_imp_lgb_ins.append(clf.feature_importance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold n°0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-28 14:05:08,061]\u001b[0m Trial 0 failed with parameters: {'n_estimators': 1000, 'learning_rate': 0.07, 'num_leaves': 640, 'max_depth': 12, 'min_split_gain': 0.4, 'subsample': 0.9, 'subsample_freq': 0.8, 'colsample_bytree': 0.7, 'alpha': 0.7000000000000001, 'lambda': 14.0} because of the following error: LightGBMError('Parameter bagging_freq should be of type int, got \"0.8\"').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<ipython-input-54-ad67c2573925>\", line 4, in <lambda>\n",
      "    func = lambda trial: objective(trial, train, label)\n",
      "  File \"<ipython-input-48-35dca9957f30>\", line 38, in objective\n",
      "    clf = lgb.train(\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\", line 1538, in _lazy_init\n",
      "    self.__init_from_np2d(data, params_str, ref_dataset)\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\", line 1659, in __init_from_np2d\n",
      "    _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n",
      "  File \"/home/panda/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\", line 125, in _safe_call\n",
      "    raise LightGBMError(_LIB.LGBM_GetLastError().decode('utf-8'))\n",
      "lightgbm.basic.LightGBMError: Parameter bagging_freq should be of type int, got \"0.8\"\n",
      "\u001b[33m[W 2023-04-28 14:05:08,229]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "LightGBMError",
     "evalue": "Parameter bagging_freq should be of type int, got \"0.8\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-ad67c2573925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstudy_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\tBest value (LogLoss): {study_lgb.best_value:.5f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \"\"\"\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-ad67c2573925>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\", sampler=optuna.samplers.GridSampler())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstudy_lgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-35dca9957f30>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, train, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnum_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         clf = lgb.train(\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;31m# construct booster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0mbooster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[1;32m   2603\u001b[0m                 )\n\u001b[1;32m   2604\u001b[0m             \u001b[0;31m# construct booster object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2605\u001b[0;31m             \u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2606\u001b[0m             \u001b[0;31m# copy the parameters from train_set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2607\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m                 \u001b[0;31m# create train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1815\u001b[0;31m                 self._lazy_init(self.data, label=self.label,\n\u001b[0m\u001b[1;32m   1816\u001b[0m                                 \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init_from_np2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__init_from_np2d\u001b[0;34m(self, mat, params_str, ref_dataset)\u001b[0m\n\u001b[1;32m   1657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mptr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_ptr_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_float_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m         _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n\u001b[0m\u001b[1;32m   1660\u001b[0m             \u001b[0mptr_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1661\u001b[0m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_ptr_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/libcityng/lib/python3.9/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLightGBMError\u001b[0m: Parameter bagging_freq should be of type int, got \"0.8\""
     ]
    }
   ],
   "source": [
    "study_lgb = optuna.create_study(direction=\"minimize\", study_name=\"LGBM Reg is_installed\", sampler=optuna.samplers.TPESampler())\n",
    "# study_lgb = optuna.create_study(direction=\"maximize\", study_name=\"LGBM Classifier\", sampler=optuna.samplers.GridSampler())\n",
    "\n",
    "func = lambda trial: objective(trial, train, label)\n",
    "study_lgb.optimize(func, n_trials=500)\n",
    "\n",
    "print(f\"\\tBest value (LogLoss): {study_lgb.best_value:.5f}\")\n",
    "print(f\"\\tBest params:\")\n",
    "\n",
    "for key, value in study_lgb.best_params.items():\n",
    "    print(f\"\\t\\t{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 4.2917, AUC: 0.8839, Precision: 0.7904, Recall: 0.3892, F1 Score: 0.5216\n"
     ]
    }
   ],
   "source": [
    "logloss = metrics.log_loss(label, [1 if i >= 0.5 else 0 for i in oof_lgb_ins], labels=[0, 1])\n",
    "acc = metrics.roc_auc_score(label, oof_lgb_ins)\n",
    "precision = metrics.precision_score(label, [1 if i >= 0.5 else 0 for i in oof_lgb_ins])\n",
    "recall = metrics.recall_score(label, [1 if i >= 0.5 else 0 for i in oof_lgb_ins])\n",
    "f1 = metrics.f1_score(label, [1 if i >= 0.5 else 0 for i in oof_lgb_ins])\n",
    "\n",
    "print(f\"Logloss: {logloss:.4f}, AUC: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:52<00:00,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_threshold_ins:  0.47999999999999976\n",
      "best_score_ins:  4.2885554717159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = []; thresholds = []\n",
    "best_score_ins = 1e30; best_threshold_ins = 0\n",
    "\n",
    "for threshold in tqdm(np.arange(0.1,0.9,0.01)):\n",
    "    # print(f'{threshold:.02f}, ',end='')\n",
    "    m = metrics.log_loss(label, np.where(oof_lgb_ins > threshold, 1, 0), labels=[0, 1])\n",
    "    scores.append(m)\n",
    "    thresholds.append(threshold)\n",
    "    if m < best_score_ins:\n",
    "        best_score_ins = m\n",
    "        best_threshold_ins = threshold\n",
    "\n",
    "print('best_threshold_ins: ', best_threshold_ins)\n",
    "print('best_score_ins: ', best_score_ins)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(thresholds,scores,'-o',color='blue')\n",
    "plt.scatter([best_threshold_ins], [best_score_ins], color='blue', s=300, alpha=1)\n",
    "plt.xlabel('Threshold',size=14)\n",
    "plt.ylabel('Validation Log Loss',size=14)\n",
    "plt.title(f'Threshold vs. Log Loss with Best Log Loss = {best_score_ins:.3f} at Best Threshold = {best_threshold_ins:.3}',size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []; thresholds = []\n",
    "# best_score_ins = 0; best_threshold_ins = 0\n",
    "\n",
    "# for threshold in tqdm(np.arange(0.1,0.9,0.01)):\n",
    "#     # print(f'{threshold:.02f}, ',end='')\n",
    "#     m = metrics.f1_score(label, np.where(oof_lgb_ins > threshold, 1, 0), average='binary')\n",
    "#     scores.append(m)\n",
    "#     thresholds.append(threshold)\n",
    "#     if m > best_score_ins:\n",
    "#         best_score_ins = m\n",
    "#         best_threshold_ins = threshold\n",
    "\n",
    "# print('best_threshold_ins: ', best_threshold_ins)\n",
    "# print('best_score_ins: ', best_score_ins)\n",
    "\n",
    "# plt.figure(figsize=(20,5))\n",
    "# plt.plot(thresholds,scores,'-o',color='blue')\n",
    "# plt.scatter([best_threshold_ins], [best_score_ins], color='blue', s=300, alpha=1)\n",
    "# plt.xlabel('Threshold',size=14)\n",
    "# plt.ylabel('Validation F1',size=14)\n",
    "# plt.title(f'Threshold vs. F1 with F1 = {best_score_ins:.3f} at Best Threshold = {best_threshold_ins:.3}',size=18)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss: 4.2805, Accuracy: 0.8843, Precision: 0.7682, Recall: 0.4122, F1 Score: 0.5365\n"
     ]
    }
   ],
   "source": [
    "logloss = metrics.log_loss(label, [1 if i >= best_threshold_ins else 0 for i in oof_lgb_ins], labels=[0, 1])\n",
    "acc = metrics.roc_auc_score(label, oof_lgb_ins)\n",
    "precision = metrics.precision_score(label, [1 if i >= best_threshold_ins else 0 for i in oof_lgb_ins])\n",
    "recall = metrics.recall_score(label, [1 if i >= best_threshold_ins else 0 for i in oof_lgb_ins])\n",
    "f1 = metrics.f1_score(label, [1 if i >= best_threshold_ins else 0 for i in oof_lgb_ins])\n",
    "\n",
    "print(f\"Logloss: {logloss:.4f}, AUC: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19324"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_installed_lgb = [1 if i >= best_threshold_ins else 0 for i in predictions_lgb_ins]\n",
    "sum(is_installed_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21983549502388514"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['is_clicked'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4251644685754754"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(is_clicked_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17401828878563977"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['is_installed'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12004497648674001"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(is_installed_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      79.000000\n",
       "mean     2390.000000\n",
       "std      2421.359227\n",
       "min         0.000000\n",
       "25%       359.000000\n",
       "50%      1792.400000\n",
       "75%      3875.200000\n",
       "max      9312.400000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_imp_ins = pd.DataFrame(feat_imp_lgb_ins, columns = features).apply(np.mean, axis = 0).sort_values(ascending=False)\n",
    "avg_imp_ins.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f_15    9312.4\n",
       "f_42    9012.0\n",
       "f_11    8785.8\n",
       "f_6     8591.8\n",
       "f_4     8244.8\n",
       "         ...  \n",
       "f_28       0.2\n",
       "f_29       0.0\n",
       "f_7        0.0\n",
       "f_27       0.0\n",
       "f_26       0.0\n",
       "Length: 79, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_imp_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission[\"RowId\"] = test_data[\"f_0\"]\n",
    "submission[\"is_clicked\"] = is_clicked_lgb\n",
    "submission[\"is_installed\"] = is_installed_lgb\n",
    "submission.to_csv('./output/lgb_42_sep_train_auc.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libcityng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
